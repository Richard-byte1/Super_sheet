{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61774577",
   "metadata": {},
   "source": [
    "Tikr market "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e83981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chrome driver...\n",
      "Navigating to TIKR login page...\n",
      "Entering login credentials...\n",
      "Submitting login form...\n",
      "Waiting for markets page to load...\n",
      "\n",
      "==============================\n",
      "Processing Most Active tab...\n",
      "==============================\n",
      "Could not find or click Most Active tab\n",
      "No data found for Most Active\n",
      "\n",
      "==============================\n",
      "Processing Gainers tab...\n",
      "==============================\n",
      "Could not find or click Gainers tab\n",
      "No data found for Gainers\n",
      "\n",
      "==============================\n",
      "Processing Losers tab...\n",
      "==============================\n",
      "Could not find or click Losers tab\n",
      "No data found for Losers\n",
      "\n",
      "==============================\n",
      "Processing Sectors tab...\n",
      "==============================\n",
      "Clicked on Sectors tab\n",
      "No data found for Sectors\n",
      "\n",
      "==============================\n",
      "Processing Regions tab...\n",
      "==============================\n",
      "Clicked on Regions tab\n",
      "No data found for Regions\n",
      "\n",
      "==============================\n",
      "Processing Assets tab...\n",
      "==============================\n",
      "Clicked on Assets tab\n",
      "No data found for Assets\n",
      "\n",
      "==============================\n",
      "Processing Factors tab...\n",
      "==============================\n",
      "Clicked on Factors tab\n",
      "No data found for Factors\n",
      "\n",
      "==================================================\n",
      "Summary of extracted data:\n",
      "==================================================\n",
      "Most Active: No data\n",
      "Gainers: No data\n",
      "Losers: No data\n",
      "Sectors: No data\n",
      "Regions: No data\n",
      "Assets: No data\n",
      "Factors: No data\n",
      "\n",
      "Data successfully exported to Excel.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import xlwings as xw\n",
    "\n",
    "# Set up Excel workbook and worksheet\n",
    "book = xw.Book('Richspread.xlsx')\n",
    "sht = book.sheets['MarketSnapshot']\n",
    "\n",
    "# Suppress warnings and logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['WDM_LOG_LEVEL'] = '0'\n",
    "\n",
    "# Define where each table should go in the Excel sheet\n",
    "table_locations = {\n",
    "    \"Most Active\": \"B2\",\n",
    "    \"Gainers\": \"G2\",\n",
    "    \"Losers\": \"L2\",\n",
    "    \"Sectors\": \"N16\",\n",
    "    \"Regions\": \"B16\",\n",
    "    \"Assets\": \"F16\",\n",
    "    \"Factors\": \"J16\"\n",
    "}\n",
    "\n",
    "def scrape_tikr_data():\n",
    "    # Configure Chrome to run in headless mode with various optimizations\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-logging\")\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "    chrome_options.add_argument(\"--silent\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\", \"enable-logging\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    # Redirect stderr to suppress selenium noise\n",
    "    original_stderr = sys.stderr\n",
    "    sys.stderr = open(os.devnull, 'w')\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        print(\"Initializing Chrome driver...\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "        print(\"Navigating to TIKR login page...\")\n",
    "        driver.get('https://app.tikr.com/markets?fid=1&ref=1p8x1t')\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        \n",
    "        # Login process - replace with your credentials\n",
    "        print(\"Entering login credentials...\")\n",
    "        email_field = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@placeholder='Enter your email']\")))\n",
    "        driver.execute_script(\"arguments[0].value = 'YOUR_EMAIL_HERE';\", email_field)\n",
    "        driver.execute_script(\"arguments[0].dispatchEvent(new Event('input', { bubbles: true }));\", email_field)\n",
    "        \n",
    "        password_field = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@type='password']\")))\n",
    "        driver.execute_script(\"arguments[0].value = 'YOUR_PASSWORD_HERE';\", password_field)\n",
    "        driver.execute_script(\"arguments[0].dispatchEvent(new Event('input', { bubbles: true }));\", password_field)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        print(\"Submitting login form...\")\n",
    "        try:\n",
    "            password_field.send_keys(Keys.ENTER)\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"Login submission failed: {e}\")\n",
    "        \n",
    "        # Give the page time to fully load after login\n",
    "        print(\"Waiting for markets page to load...\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Dictionary to store all scraped data\n",
    "        all_results = {}\n",
    "        \n",
    "        # Process the top navigation tabs (Most Active, Gainers, Losers)\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"Processing Most Active tab...\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        # Click on the Most Active tab\n",
    "        tab_clicked = driver.execute_script(\"\"\"\n",
    "            var topTabs = document.querySelectorAll('.tab, [role=\"tab\"], li');\n",
    "            for (var i = 0; i < topTabs.length; i++) {\n",
    "                if (topTabs[i].textContent.trim() === \"Most Active\") {\n",
    "                    topTabs[i].click();\n",
    "                    return true;\n",
    "                }\n",
    "            }\n",
    "            return false;\n",
    "        \"\"\")\n",
    "        \n",
    "        if not tab_clicked:\n",
    "            print(\"Could not find or click Most Active tab\")\n",
    "        \n",
    "        time.sleep(2)  # Wait for content to load\n",
    "        \n",
    "        # Extract stock data from the table\n",
    "        stocks_data = driver.execute_script(\"\"\"\n",
    "            var data = [];\n",
    "            var table = document.querySelector('table');\n",
    "            if (table) {\n",
    "                var rows = table.querySelectorAll('tr');\n",
    "                for (var i = 1; i < rows.length; i++) {  // Skip header row\n",
    "                    var cells = rows[i].querySelectorAll('td');\n",
    "                    if (cells.length >= 4) {\n",
    "                        data.push({\n",
    "                            ticker: cells[0].textContent.trim(),\n",
    "                            lastPrice: cells[1].textContent.trim(),\n",
    "                            change: cells[2].textContent.trim(),\n",
    "                            percentChange: cells[3].textContent.trim()\n",
    "                        });\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            return data;\n",
    "        \"\"\")\n",
    "        \n",
    "        if stocks_data:\n",
    "            print(\"\\nMost Active Table Output:\\n\")\n",
    "            print(f\"{'Ticker':<8} | {'Last Price':<12} | {'Change':<10} | {'% Change':<10}\")\n",
    "            print(\"-\" * 50)\n",
    "            for stock in stocks_data:\n",
    "                print(f\"{stock['ticker']:<8} | {stock['lastPrice']:<12} | {stock['change']:<10} | {stock['percentChange']:<10}\")\n",
    "            all_results[\"Most Active\"] = stocks_data\n",
    "        else:\n",
    "            print(\"No data found for Most Active\")\n",
    "            all_results[\"Most Active\"] = []\n",
    "        \n",
    "        # Process Gainers tab\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"Processing Gainers tab...\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        tab_clicked = driver.execute_script(\"\"\"\n",
    "            var topTabs = document.querySelectorAll('.tab, [role=\"tab\"], li');\n",
    "            for (var i = 0; i < topTabs.length; i++) {\n",
    "                if (topTabs[i].textContent.trim() === \"Gainers\") {\n",
    "                    topTabs[i].click();\n",
    "                    return true;\n",
    "                }\n",
    "            }\n",
    "            return false;\n",
    "        \"\"\")\n",
    "        \n",
    "        if not tab_clicked:\n",
    "            print(\"Could not find or click Gainers tab\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        stocks_data = driver.execute_script(\"\"\"\n",
    "            var data = [];\n",
    "            var table = document.querySelector('table');\n",
    "            if (table) {\n",
    "                var rows = table.querySelectorAll('tr');\n",
    "                for (var i = 1; i < rows.length; i++) {\n",
    "                    var cells = rows[i].querySelectorAll('td');\n",
    "                    if (cells.length >= 4) {\n",
    "                        data.push({\n",
    "                            ticker: cells[0].textContent.trim(),\n",
    "                            lastPrice: cells[1].textContent.trim(),\n",
    "                            change: cells[2].textContent.trim(),\n",
    "                            percentChange: cells[3].textContent.trim()\n",
    "                        });\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            return data;\n",
    "        \"\"\")\n",
    "        \n",
    "        if stocks_data:\n",
    "            print(\"\\nGainers Table Output:\\n\")\n",
    "            print(f\"{'Ticker':<8} | {'Last Price':<12} | {'Change':<10} | {'% Change':<10}\")\n",
    "            print(\"-\" * 50)\n",
    "            for stock in stocks_data:\n",
    "                print(f\"{stock['ticker']:<8} | {stock['lastPrice']:<12} | {stock['change']:<10} | {stock['percentChange']:<10}\")\n",
    "            all_results[\"Gainers\"] = stocks_data\n",
    "        else:\n",
    "            print(\"No data found for Gainers\")\n",
    "            all_results[\"Gainers\"] = []\n",
    "        \n",
    "        # Process Losers tab\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"Processing Losers tab...\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        tab_clicked = driver.execute_script(\"\"\"\n",
    "            var topTabs = document.querySelectorAll('.tab, [role=\"tab\"], li');\n",
    "            for (var i = 0; i < topTabs.length; i++) {\n",
    "                if (topTabs[i].textContent.trim() === \"Losers\") {\n",
    "                    topTabs[i].click();\n",
    "                    return true;\n",
    "                }\n",
    "            }\n",
    "            return false;\n",
    "        \"\"\")\n",
    "        \n",
    "        if not tab_clicked:\n",
    "            print(\"Could not find or click Losers tab\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        stocks_data = driver.execute_script(\"\"\"\n",
    "            var data = [];\n",
    "            var table = document.querySelector('table');\n",
    "            if (table) {\n",
    "                var rows = table.querySelectorAll('tr');\n",
    "                for (var i = 1; i < rows.length; i++) {\n",
    "                    var cells = rows[i].querySelectorAll('td');\n",
    "                    if (cells.length >= 4) {\n",
    "                        data.push({\n",
    "                            ticker: cells[0].textContent.trim(),\n",
    "                            lastPrice: cells[1].textContent.trim(),\n",
    "                            change: cells[2].textContent.trim(),\n",
    "                            percentChange: cells[3].textContent.trim()\n",
    "                        });\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            return data;\n",
    "        \"\"\")\n",
    "        \n",
    "        if stocks_data:\n",
    "            print(\"\\nLosers Table Output:\\n\")\n",
    "            print(f\"{'Ticker':<8} | {'Last Price':<12} | {'Change':<10} | {'% Change':<10}\")\n",
    "            print(\"-\" * 50)\n",
    "            for stock in stocks_data:\n",
    "                print(f\"{stock['ticker']:<8} | {stock['lastPrice']:<12} | {stock['change']:<10} | {stock['percentChange']:<10}\")\n",
    "            all_results[\"Losers\"] = stocks_data\n",
    "        else:\n",
    "            print(\"No data found for Losers\")\n",
    "            all_results[\"Losers\"] = []\n",
    "        \n",
    "        # Process the bottom navigation tabs (Sectors, Regions, Assets, Factors)\n",
    "        # Define expected content for each tab to help with validation\n",
    "        bottom_tabs = [\n",
    "            {\n",
    "                \"name\": \"Sectors\", \n",
    "                \"expected_items\": [\"Technology\", \"Health Care\", \"Financials\", \"Energy\", \"Real Estate\", \n",
    "                                 \"Consumer Discretionary\", \"Consumer Staples\", \"Industrials\", \"Materials\", \n",
    "                                 \"Utilities\", \"Communication Services\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Regions\", \n",
    "                \"expected_items\": [\"Developed ex-US\", \"Emerging Markets\", \"Canada\", \"United Kingdom\", \n",
    "                                 \"China\", \"Japan\", \"Germany\", \"Brazil\", \"India\", \"Australia\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Assets\", \n",
    "                \"expected_items\": [\"Commodities\", \"Gold\", \"Silver\", \"Oil\", \"Natural Gas\", \n",
    "                                 \"U.S. Treasuries\", \"Municipals\", \"TIPS\", \"EM Govt Bonds\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Factors\", \n",
    "                \"expected_items\": [\"Value\", \"Growth\", \"Momentum\", \"Quality\", \"Size\", \"Dividend\", \n",
    "                                 \"ESG\", \"High Dividend\", \"International Value\"]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process each bottom tab\n",
    "        for tab_info in bottom_tabs:\n",
    "            tab_name = tab_info[\"name\"]\n",
    "            expected_items = tab_info[\"expected_items\"]\n",
    "            \n",
    "            print(f\"\\n{'='*30}\")\n",
    "            print(f\"Processing {tab_name} tab...\")\n",
    "            print(f\"{'='*30}\")\n",
    "            \n",
    "            # Click on the tab\n",
    "            driver.execute_script(\"\"\"\n",
    "                var tabContainers = document.querySelectorAll('[role=\"tablist\"], .tabs, .tab-container, .nav');\n",
    "                var bottomTabContainer = null;\n",
    "                for (var i = 0; i < tabContainers.length; i++) {\n",
    "                    var text = tabContainers[i].textContent;\n",
    "                    if (text.includes('Sectors') && text.includes('Regions') && text.includes('Assets')) {\n",
    "                        bottomTabContainer = tabContainers[i];\n",
    "                        break;\n",
    "                    }\n",
    "                }\n",
    "                if (bottomTabContainer) {\n",
    "                    var tabs = bottomTabContainer.querySelectorAll('[role=\"tab\"], .tab, li');\n",
    "                    for (var i = 0; i < tabs.length; i++) {\n",
    "                        if (tabs[i].textContent.trim() === arguments[0]) {\n",
    "                            tabs[i].click();\n",
    "                            return true;\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                return false;\n",
    "            \"\"\", tab_name)\n",
    "            \n",
    "            print(f\"Clicked on {tab_name} tab\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Extract data from the table and validate it\n",
    "            data = driver.execute_script(\"\"\"\n",
    "                var data = [];\n",
    "                var expectedItems = arguments[1];\n",
    "                var tables = document.querySelectorAll('table');\n",
    "                var targetTable = null;\n",
    "                \n",
    "                // Find the correct table by checking for expected content\n",
    "                for (var t = 0; t < tables.length; t++) {\n",
    "                    var table = tables[t];\n",
    "                    var tableText = table.textContent;\n",
    "                    var matchCount = 0;\n",
    "                    \n",
    "                    for (var e = 0; e < expectedItems.length; e++) {\n",
    "                        if (tableText.includes(expectedItems[e])) {\n",
    "                            matchCount++;\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    if (matchCount >= 2) {\n",
    "                        targetTable = table;\n",
    "                        break;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                if (targetTable) {\n",
    "                    var rows = targetTable.querySelectorAll('tr');\n",
    "                    for (var i = 1; i < rows.length; i++) {\n",
    "                        var cells = rows[i].querySelectorAll('td');\n",
    "                        if (cells.length >= 3) {\n",
    "                            var name = cells[0].textContent.trim();\n",
    "                            if (name && name !== 'ETF' && name !== 'Name' && !name.includes('Last Price')) {\n",
    "                                data.push({\n",
    "                                    name: name,\n",
    "                                    lastPrice: cells[1].textContent.trim(),\n",
    "                                    change: cells[2].textContent.trim()\n",
    "                                });\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                return data;\n",
    "            \"\"\", tab_name, expected_items)\n",
    "            \n",
    "            if data and len(data) > 0:\n",
    "                print(f\"\\n{tab_name} Table Output:\\n\")\n",
    "                print(f\"{'Name':<30} | {'Last Price':<12} | {'% Change':<10}\")\n",
    "                print(\"-\" * 55)\n",
    "                for item in data[:10]:  # Show first 10 items\n",
    "                    print(f\"{item['name']:<30} | {item['lastPrice']:<12} | {item['change']:<10}\")\n",
    "                all_results[tab_name] = data\n",
    "            else:\n",
    "                print(f\"No data found for {tab_name}\")\n",
    "                all_results[tab_name] = []\n",
    "        \n",
    "        # Export all data to Excel\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Summary of extracted data:\")\n",
    "        print(\"=\"*50)\n",
    "        for tab_name, data in all_results.items():\n",
    "            if data:\n",
    "                print(f\"{tab_name}: {len(data)} items\")\n",
    "            else:\n",
    "                print(f\"{tab_name}: No data\")\n",
    "        \n",
    "        # Write each table to its designated location in Excel\n",
    "        for tab_name, data in all_results.items():\n",
    "            if not data or tab_name not in table_locations:\n",
    "                continue\n",
    "            \n",
    "            start_cell = table_locations[tab_name]\n",
    "            \n",
    "            # Format data based on table type\n",
    "            if tab_name in [\"Most Active\", \"Gainers\", \"Losers\"]:\n",
    "                headers = [\"ticker\", \"lastPrice\", \"change\", \"percentChange\"]\n",
    "                rows = [[item.get(\"ticker\", \"\"), item.get(\"lastPrice\", \"\"), \n",
    "                        item.get(\"change\", \"\"), item.get(\"percentChange\", \"\")] for item in data]\n",
    "            else:\n",
    "                headers = [\"name\", \"lastPrice\", \"change\"]\n",
    "                rows = [[item.get(\"name\", \"\"), item.get(\"lastPrice\", \"\"), \n",
    "                        item.get(\"change\", \"\")] for item in data]\n",
    "            \n",
    "            # Write to Excel\n",
    "            sht.range(start_cell).value = [headers] + rows\n",
    "        \n",
    "        print(\"\\nData successfully exported to Excel.\")\n",
    "        book.save()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # Restore stderr and cleanup\n",
    "        sys.stderr = original_stderr\n",
    "        if driver is not None:\n",
    "            driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_tikr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260515f",
   "metadata": {},
   "source": [
    "Money Market "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feae5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import xlwings as xw\n",
    "import re\n",
    "\n",
    "def scrape_mmf_data():\n",
    "    \"\"\"\n",
    "    Scrapes money market fund assets data from ICI.org and exports to Excel.\n",
    "    The data includes Government, Retail, and Institutional fund categories.\n",
    "    \"\"\"\n",
    "    url = \"https://www.ici.org/research/stats/mmf\"\n",
    "    print(f\"Fetching data from {url}...\")\n",
    "\n",
    "    # Set headers to mimic a real browser request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Try to find the money market funds table\n",
    "        table = None\n",
    "        \n",
    "        # First, look for a heading that mentions \"Assets of Money Market Funds\"\n",
    "        heading = soup.find(lambda tag: tag.name in ['h1', 'h2', 'h3', 'h4', 'h5'] and \n",
    "                            \"Assets of Money Market Funds\" in tag.text)\n",
    "        if heading:\n",
    "            table = heading.find_next('table')\n",
    "        \n",
    "        # If that doesn't work, search all tables for one containing our expected data\n",
    "        if not table:\n",
    "            tables = soup.find_all('table')\n",
    "            for t in tables:\n",
    "                if \"Government\" in t.text and \"Retail\" in t.text and \"Institutional\" in t.text:\n",
    "                    table = t\n",
    "                    break\n",
    "        \n",
    "        if not table:\n",
    "            print(\"Could not find the Money Market Funds table.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Table found! Extracting data...\")\n",
    "\n",
    "        # Extract the header row\n",
    "        header_row = table.find('tr')\n",
    "        headers_row = [th.text.strip() for th in header_row.find_all(['th', 'td'])] if header_row else []\n",
    "        data_rows = table.find_all('tr')[1:] if headers_row else table.find_all('tr')\n",
    "\n",
    "        # Extract all data rows\n",
    "        rows = []\n",
    "        for row in data_rows:\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            if cells:\n",
    "                row_data = [cell.text.strip() for cell in cells]\n",
    "                rows.append(row_data)\n",
    "\n",
    "        # Create DataFrame from the scraped data\n",
    "        df = pd.DataFrame(rows)\n",
    "        if headers_row and len(headers_row) == df.shape[1]:\n",
    "            df.columns = headers_row\n",
    "        else:\n",
    "            # If headers don't match, create generic column names\n",
    "            df.columns = ['Column_' + str(i) for i in range(df.shape[1])]\n",
    "\n",
    "        # Convert numeric columns (skip the first column which is usually text)\n",
    "        for col in df.columns[1:]:\n",
    "            try:\n",
    "                df[col] = df[col].str.replace(',', '').astype(float)\n",
    "            except Exception:\n",
    "                # If conversion fails, leave as string\n",
    "                pass\n",
    "\n",
    "        # Only keep the first 5 rows of data\n",
    "        if len(df) > 5:\n",
    "            df = df.iloc[:5]\n",
    "\n",
    "        # Clean up date column headers to MM/DD/YYYY format\n",
    "        date_cols = [col for col in df.columns if re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', str(col))]\n",
    "        for col in date_cols:\n",
    "            parts = str(col).split('/')\n",
    "            if len(parts) == 3:\n",
    "                month = parts[0].zfill(2)\n",
    "                day = parts[1].zfill(2)\n",
    "                year = parts[2]\n",
    "                new_col = f\"{month}/{day}/{year}\"\n",
    "                if col != new_col:\n",
    "                    df.rename(columns={col: new_col}, inplace=True)\n",
    "\n",
    "        # Find all date columns again after cleaning\n",
    "        date_cols = [col for col in df.columns if re.match(r'\\d{2}/\\d{2}/\\d{4}', str(col))]\n",
    "        date_cols_sorted = sorted(date_cols, key=lambda x: datetime.strptime(x, '%m/%d/%Y'))\n",
    "\n",
    "        # Convert date format from US (MM/DD/YYYY) to Irish (DD/MM/YYYY)\n",
    "        for col in date_cols_sorted:\n",
    "            dt = datetime.strptime(col, '%m/%d/%Y')\n",
    "            new_col = dt.strftime('%d/%m/%Y')\n",
    "            if col != new_col:\n",
    "                df.rename(columns={col: new_col}, inplace=True)\n",
    "\n",
    "        # Reorder columns: non-date columns first, then dates in chronological order, then change column\n",
    "        date_cols_irish = [col for col in df.columns if re.match(r'\\d{2}/\\d{2}/\\d{4}', str(col))]\n",
    "        date_cols_irish_sorted = sorted(date_cols_irish, key=lambda x: datetime.strptime(x, '%d/%m/%Y'))\n",
    "\n",
    "        non_date_cols = [col for col in df.columns if col not in date_cols_irish and col != '$ Change*']\n",
    "        change_col = ['$ Change*'] if '$ Change*' in df.columns else []\n",
    "        new_order = non_date_cols + date_cols_irish_sorted + change_col\n",
    "\n",
    "        df = df[new_order]\n",
    "\n",
    "        # Add timestamp for tracking when data was scraped\n",
    "        current_time = datetime.now().strftime('%d/%m/%Y %H:%M:%S')\n",
    "        print(f\"Data extracted successfully at {current_time}\")\n",
    "\n",
    "        # Show a preview of what we got\n",
    "        print(\"\\nData Preview:\")\n",
    "        print(df)\n",
    "\n",
    "        # Export to Excel file\n",
    "        try:\n",
    "            excel_path = \"Richspread.xlsx\"\n",
    "            sheet_name = \"MarketSnapshot\"\n",
    "            try:\n",
    "                book = xw.Book(excel_path)\n",
    "            except Exception:\n",
    "                print(f\"Excel file '{excel_path}' not found or cannot be opened.\")\n",
    "                return df\n",
    "            \n",
    "            sht = book.sheets[sheet_name]\n",
    "            sht.range(\"B40\").options(index=False).value = df\n",
    "            print(f\"Data exported to {excel_path} in sheet '{sheet_name}' at cell B40\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting to Excel: {e}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Money Market Fund Assets Scraper\")\n",
    "    print(\"=\" * 40)\n",
    "    scrape_mmf_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d41a70",
   "metadata": {},
   "source": [
    "treasury and sofr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de1c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatham Financial Market Rates Scraper\n",
      "=============================================\n",
      "Starting browser and navigating to Chatham Financial...\n",
      "Page loaded, waiting for dynamic content...\n",
      "Extracting U.S. Treasuries data...\n",
      "Extracting SOFR data...\n",
      "\n",
      "U.S. Treasuries Rates:\n",
      "           17 Jul 2025 18 Jun 2025 18 Jul 2024\n",
      "0   1 Year      4.102%      4.086%      4.865%\n",
      "1   2 Year      3.914%      3.932%      4.469%\n",
      "2   3 Year      3.885%      3.884%      4.236%\n",
      "3   5 Year      3.998%      3.978%      4.115%\n",
      "4   7 Year      4.217%      4.165%      4.134%\n",
      "5  10 Year      4.456%      4.382%      4.195%\n",
      "6  30 Year      5.006%      4.881%      4.411%\n",
      "\n",
      "SOFR Rates:\n",
      "                       16 Jul 2025 17 Jun 2025 17 Jul 2024\n",
      "0                 SOFR    4.34000%    4.31000%    5.35000%\n",
      "1  30-Day Average SOFR    4.34732%    4.30185%    5.34343%\n",
      "2  90-Day Average SOFR    4.34107%    4.34210%    5.35700%\n",
      "3    1-month Term SOFR    4.34979%    4.31758%    5.34119%\n",
      "4    3-month Term SOFR    4.32900%    4.31890%    5.27947%\n",
      "U.S. Treasuries data exported to Excel\n",
      "SOFR data exported to Excel\n",
      "All data exported to Richspread.xlsx in sheet 'MarketSnapshot' starting at cell B50\n",
      "Browser closed\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import xlwings as xw\n",
    "\n",
    "def scrape_chatham_market_rates():\n",
    "    \"\"\"\n",
    "    Scrapes U.S. Treasury and SOFR rates from Chatham Financial's market rates page.\n",
    "    Exports the data to an Excel file for analysis.\n",
    "    \"\"\"\n",
    "    url = \"https://www.chathamfinancial.com/technology/us-market-rates\"\n",
    "    \n",
    "    # Configure Chrome to run in headless mode (no visible browser window)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    print(\"Starting browser and navigating to Chatham Financial...\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        print(\"Page loaded, waiting for dynamic content...\")\n",
    "        time.sleep(5)  # Give the page time to load all dynamic content\n",
    "        \n",
    "        def scrape_table(table_xpath):\n",
    "            \"\"\"\n",
    "            Helper function to extract table data using XPath.\n",
    "            Returns a list of lists containing the table data.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                table = driver.find_element(By.XPATH, table_xpath)\n",
    "                rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                data = []\n",
    "                \n",
    "                for row in rows:\n",
    "                    # Get both header cells (th) and data cells (td)\n",
    "                    cells = row.find_elements(By.TAG_NAME, \"th\") + row.find_elements(By.TAG_NAME, \"td\")\n",
    "                    data.append([cell.text for cell in cells])\n",
    "                \n",
    "                return data\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping table: {e}\")\n",
    "                return []\n",
    "        \n",
    "        # XPaths to find the specific tables we want\n",
    "        us_treasuries_xpath = \"//h2[contains(text(), 'U.S. Treasuries')]/following::table[1]\"\n",
    "        sofr_xpath = \"//h2[contains(text(), 'Secured Overnight Financing Rate')]/following::table[1]\"\n",
    "        \n",
    "        print(\"Extracting U.S. Treasuries data...\")\n",
    "        us_treasuries_data = scrape_table(us_treasuries_xpath)\n",
    "        \n",
    "        print(\"Extracting SOFR data...\")\n",
    "        sofr_data = scrape_table(sofr_xpath)\n",
    "        \n",
    "        # Convert the scraped data to pandas DataFrames\n",
    "        if us_treasuries_data and len(us_treasuries_data) > 1:\n",
    "            us_treasuries_df = pd.DataFrame(us_treasuries_data[1:], columns=us_treasuries_data[0])\n",
    "        else:\n",
    "            print(\"No U.S. Treasuries data found\")\n",
    "            us_treasuries_df = pd.DataFrame()\n",
    "        \n",
    "        if sofr_data and len(sofr_data) > 1:\n",
    "            sofr_df = pd.DataFrame(sofr_data[1:], columns=sofr_data[0])\n",
    "        else:\n",
    "            print(\"No SOFR data found\")\n",
    "            sofr_df = pd.DataFrame()\n",
    "        \n",
    "        # Display the scraped data\n",
    "        if not us_treasuries_df.empty:\n",
    "            print(\"\\nU.S. Treasuries Rates:\")\n",
    "            print(us_treasuries_df)\n",
    "        \n",
    "        if not sofr_df.empty:\n",
    "            print(\"\\nSOFR Rates:\")\n",
    "            print(sofr_df)\n",
    "        \n",
    "        # Export to Excel\n",
    "        excel_path = \"Richspread.xlsx\"\n",
    "        sheet_name = \"MarketSnapshot\"\n",
    "        \n",
    "        try:\n",
    "            book = xw.Book(excel_path)\n",
    "            sht = book.sheets[sheet_name]\n",
    "            \n",
    "            # Add a header for the section\n",
    "            sht.range(\"B49\").value = \"Treasury and SOFR Rates\"\n",
    "            \n",
    "            # Export U.S. Treasuries data\n",
    "            if not us_treasuries_df.empty:\n",
    "                sht.range(\"B50\").options(index=False).value = us_treasuries_df\n",
    "                print(\"U.S. Treasuries data exported to Excel\")\n",
    "            \n",
    "            # Export SOFR data below the Treasuries table with a gap\n",
    "            if not sofr_df.empty:\n",
    "                next_row = \"B{}\".format(50 + len(us_treasuries_df) + 2)\n",
    "                sht.range(next_row).options(index=False).value = sofr_df\n",
    "                print(\"SOFR data exported to Excel\")\n",
    "            \n",
    "            print(f\"All data exported to {excel_path} in sheet '{sheet_name}' starting at cell B50\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Excel file '{excel_path}' not found or cannot be opened: {e}\")\n",
    "            return us_treasuries_df, sofr_df\n",
    "        \n",
    "        return us_treasuries_df, sofr_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    finally:\n",
    "        # Always close the browser, even if there's an error\n",
    "        driver.quit()\n",
    "        print(\"Browser closed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatham Financial Market Rates Scraper\")\n",
    "    print(\"=\" * 45)\n",
    "    scrape_chatham_market_rates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ecc3a",
   "metadata": {},
   "source": [
    "Eco Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ac06dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economic Calendar Scraper\n",
      "==============================\n",
      "Starting Chrome browser...\n",
      "Navigating to Trading Economics Calendar...\n",
      "Waiting for calendar data to load...\n",
      "Extracting economic events...\n",
      "Found 82 rows in calendar table\n",
      "Found date header: Tuesday July 22 2025\n",
      "Event: RBA Meeting Minutes\n",
      "Raw values - Actual: , Previous: \n",
      "Converted - Actual: None, Previous: None\n",
      "Event: Fed Chair Powell Speech\n",
      "Raw values - Actual: , Previous: \n",
      "Converted - Actual: None, Previous: None\n",
      "Found date header: Wednesday July 23 2025\n",
      "Event: Existing Home Sales JUN\n",
      "Raw values - Actual: , Previous: 4.03M\n",
      "Converted - Actual: None, Previous: 4030000.0000000005\n",
      "Found date header: Thursday July 24 2025\n",
      "Event: GfK Consumer Confidence AUG\n",
      "Raw values - Actual: , Previous: -20.3\n",
      "Converted - Actual: None, Previous: -20.3\n",
      "Event: HCOB Manufacturing PMI Flash JUL\n",
      "Raw values - Actual: , Previous: 49.0\n",
      "Converted - Actual: None, Previous: 49.0\n",
      "Event: S&P Global Manufacturing PMI Flash JUL\n",
      "Raw values - Actual: , Previous: 47.7\n",
      "Converted - Actual: None, Previous: 47.7\n",
      "Event: S&P Global Services PMI Flash JUL\n",
      "Raw values - Actual: , Previous: 52.8\n",
      "Converted - Actual: None, Previous: 52.8\n",
      "Event: Deposit Facility Rate\n",
      "Raw values - Actual: , Previous: 2%\n",
      "Converted - Actual: None, Previous: 0.02\n",
      "Event: ECB Interest Rate Decision\n",
      "Raw values - Actual: , Previous: 2.15%\n",
      "Converted - Actual: None, Previous: 0.0215\n",
      "Event: ECB Press Conference\n",
      "Raw values - Actual: , Previous: \n",
      "Converted - Actual: None, Previous: None\n",
      "Found date header: Friday July 25 2025\n",
      "Event: Retail Sales MoM JUN\n",
      "Raw values - Actual: , Previous: -2.7%\n",
      "Converted - Actual: None, Previous: -0.027000000000000003\n",
      "Event: Ifo Business Climate JUL\n",
      "Raw values - Actual: , Previous: 88.4\n",
      "Converted - Actual: None, Previous: 88.4\n",
      "Event: Durable Goods Orders MoM JUN\n",
      "Raw values - Actual: , Previous: 16.4%\n",
      "Converted - Actual: None, Previous: 0.16399999999999998\n",
      "Found date header: Tuesday July 29 2025\n",
      "Event: GDP Growth Rate QoQ Flash Q2\n",
      "Raw values - Actual: , Previous: 0.6%\n",
      "Converted - Actual: None, Previous: 0.006\n",
      "Event: GDP Growth Rate YoY Flash Q2\n",
      "Raw values - Actual: , Previous: 2.8%\n",
      "Converted - Actual: None, Previous: 0.027999999999999997\n",
      "Event: JOLTs Job Openings JUN\n",
      "Raw values - Actual: , Previous: 7.769M\n",
      "Converted - Actual: None, Previous: 7769000.0\n",
      "Found date header: Wednesday July 30 2025\n",
      "Event: GDP Growth Rate QoQ Prel Q2\n",
      "Raw values - Actual: , Previous: 0.1%\n",
      "Converted - Actual: None, Previous: 0.001\n",
      "Event: GDP Growth Rate YoY Prel Q2\n",
      "Raw values - Actual: , Previous: 0.6%\n",
      "Converted - Actual: None, Previous: 0.006\n",
      "Successfully extracted 18 economic events.\n",
      "Creating pandas DataFrame...\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       18 non-null     object \n",
      " 1   Time       18 non-null     object \n",
      " 2   Country    18 non-null     object \n",
      " 3   Event      18 non-null     object \n",
      " 4   Actual     0 non-null      float64\n",
      " 5   Previous   15 non-null     float64\n",
      " 6   Consensus  10 non-null     float64\n",
      " 7   Forecast   15 non-null     float64\n",
      "dtypes: float64(4), object(4)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "\n",
      "First 10 rows:\n",
      "                     Date      Time Country  \\\n",
      "0    Tuesday July 22 2025  02:30 AM      AU   \n",
      "1    Tuesday July 22 2025  01:30 PM      US   \n",
      "2  Wednesday July 23 2025  03:00 PM      US   \n",
      "3   Thursday July 24 2025  07:00 AM      DE   \n",
      "4   Thursday July 24 2025  08:30 AM      DE   \n",
      "5   Thursday July 24 2025  09:30 AM      GB   \n",
      "6   Thursday July 24 2025  09:30 AM      GB   \n",
      "7   Thursday July 24 2025  01:15 PM      EA   \n",
      "8   Thursday July 24 2025  01:15 PM      EA   \n",
      "9   Thursday July 24 2025  01:45 PM      EA   \n",
      "\n",
      "                                    Event  Actual      Previous     Consensus  \\\n",
      "0                     RBA Meeting Minutes     NaN           NaN           NaN   \n",
      "1                 Fed Chair Powell Speech     NaN           NaN           NaN   \n",
      "2                 Existing Home Sales JUN     NaN  4.030000e+06  4.000000e+06   \n",
      "3             GfK Consumer Confidence AUG     NaN -2.030000e+01 -1.900000e+01   \n",
      "4        HCOB Manufacturing PMI Flash JUL     NaN  4.900000e+01  4.940000e+01   \n",
      "5  S&P Global Manufacturing PMI Flash JUL     NaN  4.770000e+01  4.800000e+01   \n",
      "6       S&P Global Services PMI Flash JUL     NaN  5.280000e+01  5.300000e+01   \n",
      "7                   Deposit Facility Rate     NaN  2.000000e-02  2.000000e-02   \n",
      "8              ECB Interest Rate Decision     NaN  2.150000e-02  2.150000e-02   \n",
      "9                    ECB Press Conference     NaN           NaN           NaN   \n",
      "\n",
      "       Forecast  \n",
      "0           NaN  \n",
      "1           NaN  \n",
      "2  4.000000e+06  \n",
      "3 -2.000000e+01  \n",
      "4  4.950000e+01  \n",
      "5  4.850000e+01  \n",
      "6  5.290000e+01  \n",
      "7  2.000000e-02  \n",
      "8  2.150000e-02  \n",
      "9           NaN  \n",
      "\n",
      "DataFrame shape: (18, 8)\n",
      "\n",
      "Exporting to Excel with formatting...\n",
      "An error occurred: Unknown property, element or command: 'Borders'\n",
      "Browser closed.\n",
      "No data was returned. Check the error messages above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.13/site-packages/aeosa/appscript/reference.py\", line 596, in __getattr__\n",
      "    selectortype, code = self.AS_appdata.referencebyname()[name]\n",
      "                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "KeyError: 'Borders'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rx/whpr72693f3cgk151m8h7fh80000gn/T/ipykernel_93503/1837625719.py\", line 199, in scrape_economic_calendar\n",
      "    table_range.api.Borders(border_id).LineStyle = 1\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.13/site-packages/aeosa/appscript/reference.py\", line 598, in __getattr__\n",
      "    raise AttributeError(\"Unknown property, element or command: {!r}\".format(name)) from e\n",
      "AttributeError: Unknown property, element or command: 'Borders'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xlwings as xw\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "def scrape_economic_calendar():\n",
    "    \"\"\"\n",
    "    Scrapes high-importance economic events from Trading Economics calendar.\n",
    "    Extracts dates, times, countries, events, and economic indicators.\n",
    "    Exports formatted data to Excel with proper styling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure Chrome for headless operation with anti-detection measures\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        print(\"Starting Chrome browser...\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        # Hide the fact that we're using automation\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        print(\"Navigating to Trading Economics Calendar...\")\n",
    "        driver.get('https://tradingeconomics.com/calendar?importance=3')\n",
    "        print(\"Waiting for calendar data to load...\")\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"calendar\")))\n",
    "        time.sleep(5)\n",
    "        print(\"Extracting economic events...\")\n",
    "        \n",
    "        # Take a screenshot to verify the page loaded correctly\n",
    "        driver.save_screenshot('page_loaded.png')\n",
    "        \n",
    "        # Get the main calendar table\n",
    "        calendar_table = driver.find_element(By.ID, \"calendar\")\n",
    "        \n",
    "        def convert_to_number(text_value):\n",
    "            \"\"\"\n",
    "            Converts text values to appropriate numeric types.\n",
    "            Handles percentages, K/M/B suffixes, and plain numbers.\n",
    "            \"\"\"\n",
    "            if not text_value or text_value == \"-\" or text_value == \"â€¦\":\n",
    "                return None\n",
    "                \n",
    "            text_value = text_value.strip()\n",
    "            \n",
    "            # Handle percentage values (e.g., \"2.5%\" -> 0.025)\n",
    "            if \"%\" in text_value:\n",
    "                try:\n",
    "                    return float(text_value.replace(\"%\", \"\")) / 100\n",
    "                except ValueError:\n",
    "                    return text_value\n",
    "            \n",
    "            # Handle values with K, M, B suffixes (thousands, millions, billions)\n",
    "            if any(suffix in text_value for suffix in [\"K\", \"M\", \"B\"]):\n",
    "                try:\n",
    "                    match = re.match(r\"(-?\\d+\\.?\\d*)([KMB])\", text_value)\n",
    "                    if match:\n",
    "                        num, suffix = match.groups()\n",
    "                        num = float(num)\n",
    "                        if suffix == \"K\":\n",
    "                            return num * 1000\n",
    "                        elif suffix == \"M\":\n",
    "                            return num * 1000000\n",
    "                        elif suffix == \"B\":\n",
    "                            return num * 1000000000\n",
    "                    return text_value\n",
    "                except:\n",
    "                    return text_value\n",
    "                    \n",
    "            # Try to convert plain numbers\n",
    "            try:\n",
    "                if text_value.isdigit():\n",
    "                    return int(text_value)\n",
    "                return float(text_value)\n",
    "            except ValueError:\n",
    "                return text_value\n",
    "        \n",
    "        all_data = []\n",
    "        current_date = None\n",
    "        \n",
    "        # Extract all rows from the calendar table\n",
    "        rows = calendar_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "        print(f\"Found {len(rows)} rows in calendar table\")\n",
    "        \n",
    "        for row in rows:\n",
    "            # Check if this row contains a date header\n",
    "            date_cells = row.find_elements(By.XPATH, \"./th[contains(@style, 'text-align: left')]\")\n",
    "            if date_cells and date_cells[0].text.strip():\n",
    "                current_date = date_cells[0].text.strip()\n",
    "                print(f\"Found date header: {current_date}\")\n",
    "                continue\n",
    "            \n",
    "            # Process event data rows\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            if len(cells) < 5:  # Skip rows without enough data\n",
    "                continue\n",
    "            \n",
    "            # Check if this is an actual event row (has time in first column)\n",
    "            time_cell = cells[0].text.strip() if cells else \"\"\n",
    "            if not time_cell or not any(x in time_cell for x in [\"AM\", \"PM\"]):\n",
    "                continue\n",
    "            \n",
    "            # Extract event information\n",
    "            event_data = {\n",
    "                'Date': current_date,\n",
    "                'Time': time_cell,\n",
    "                'Country': cells[1].text.strip() if len(cells) > 1 else \"\",\n",
    "                'Event': cells[4].text.strip() if len(cells) > 4 else \"\"\n",
    "            }\n",
    "            \n",
    "            # Extract economic indicator values\n",
    "            actual_value = cells[5].text.strip() if len(cells) > 5 else \"\"\n",
    "            previous_value = cells[6].text.strip() if len(cells) > 6 else \"\"\n",
    "            consensus_value = cells[7].text.strip() if len(cells) > 7 else \"\"\n",
    "            forecast_value = cells[8].text.strip() if len(cells) > 8 else \"\"\n",
    "            \n",
    "            # Convert text values to appropriate numeric types\n",
    "            event_data['Actual'] = convert_to_number(actual_value)\n",
    "            event_data['Previous'] = convert_to_number(previous_value)\n",
    "            event_data['Consensus'] = convert_to_number(consensus_value)\n",
    "            event_data['Forecast'] = convert_to_number(forecast_value)\n",
    "            \n",
    "            # Debug output for data validation\n",
    "            print(f\"Event: {event_data['Event']}\")\n",
    "            print(f\"Raw values - Actual: {actual_value}, Previous: {previous_value}\")\n",
    "            print(f\"Converted - Actual: {event_data['Actual']}, Previous: {event_data['Previous']}\")\n",
    "            \n",
    "            # Only add events with valid data\n",
    "            if event_data['Event'] and event_data['Date']:\n",
    "                all_data.append(event_data)\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data was extracted. Taking screenshot for debugging...\")\n",
    "            driver.save_screenshot('no_data_extracted.png')\n",
    "            return\n",
    "\n",
    "        print(f\"Successfully extracted {len(all_data)} economic events.\")\n",
    "        \n",
    "        # Create and format the DataFrame\n",
    "        print(\"Creating pandas DataFrame...\")\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Convert numeric columns properly\n",
    "        numeric_cols = ['Actual', 'Previous', 'Consensus', 'Forecast']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Set the desired column order\n",
    "        column_order = [\"Date\", \"Time\", \"Country\", \"Event\", \"Actual\", \"Previous\", \"Consensus\", \"Forecast\"]\n",
    "        df = df[column_order]\n",
    "        \n",
    "        # Display DataFrame information for verification\n",
    "        print(\"\\nDataFrame Info:\")\n",
    "        print(df.info())\n",
    "        print(\"\\nFirst 10 rows:\")\n",
    "        print(df.head(10))\n",
    "        print(\"\\nDataFrame shape:\", df.shape)\n",
    "        \n",
    "        # Export to Excel with professional formatting\n",
    "        print(\"\\nExporting to Excel with formatting...\")\n",
    "        \n",
    "        # Open the Excel workbook\n",
    "        book = xw.Book('Richspread.xlsx')\n",
    "        sht = book.sheets['MarketSnapshot']\n",
    "        \n",
    "        # Clear the target area before writing new data\n",
    "        end_row = 70 + len(df) + 5\n",
    "        sht.range(f'B70:J{end_row}').clear()\n",
    "        \n",
    "        # Write column headers\n",
    "        headers = list(df.columns)\n",
    "        sht.range('B70').value = headers\n",
    "        \n",
    "        # Write the actual data\n",
    "        data_values = df.values.tolist()\n",
    "        if data_values:\n",
    "            sht.range('B71').value = data_values\n",
    "        \n",
    "        # Apply professional table formatting\n",
    "        table_range = sht.range(f'B70:I{70 + len(df)}')\n",
    "        \n",
    "        # Add borders to the entire table\n",
    "        for border_id in range(7, 13):\n",
    "            table_range.api.Borders(border_id).LineStyle = 1\n",
    "            table_range.api.Borders(border_id).Weight = 2\n",
    "        \n",
    "        # Format the header row\n",
    "        header_range = sht.range('B70:I70')\n",
    "        header_range.api.Font.Bold = True\n",
    "        header_range.api.Interior.Color = 0xF0F0F0  # Light gray background\n",
    "        header_range.api.HorizontalAlignment = -4108  # Center alignment\n",
    "        \n",
    "        # Set column-specific formatting\n",
    "        sht.range(f'B71:B{70 + len(df)}').api.HorizontalAlignment = -4131  # Date - Left align\n",
    "        sht.range(f'C71:C{70 + len(df)}').api.HorizontalAlignment = -4108  # Time - Center align\n",
    "        sht.range(f'D71:D{70 + len(df)}').api.HorizontalAlignment = -4108  # Country - Center align\n",
    "        sht.range(f'E71:E{70 + len(df)}').api.HorizontalAlignment = -4131  # Event - Left align\n",
    "        sht.range(f'F71:I{70 + len(df)}').api.HorizontalAlignment = -4152  # Numeric - Right align\n",
    "        \n",
    "        # Format numeric values with appropriate number formats\n",
    "        for idx, row in df.iterrows():\n",
    "            excel_row = 71 + idx\n",
    "            for col_idx, col in enumerate(['Actual', 'Previous', 'Consensus', 'Forecast']):\n",
    "                col_letter = chr(ord('F') + col_idx)  # F, G, H, I\n",
    "                cell_value = row[col]\n",
    "                \n",
    "                if pd.notna(cell_value) and isinstance(cell_value, (int, float)):\n",
    "                    # Format small decimals as percentages, others as numbers\n",
    "                    if abs(cell_value) < 1 and cell_value != 0:\n",
    "                        sht.range(f'{col_letter}{excel_row}').number_format = '0.00%'\n",
    "                    else:\n",
    "                        sht.range(f'{col_letter}{excel_row}').number_format = '#,##0.00'\n",
    "        \n",
    "        # Auto-fit all columns and set minimum widths\n",
    "        sht.range('B:I').columns.autofit()\n",
    "        sht.range('B:B').api.ColumnWidth = 20  # Date column\n",
    "        sht.range('C:C').api.ColumnWidth = 10  # Time column\n",
    "        sht.range('D:D').api.ColumnWidth = 8   # Country column\n",
    "        sht.range('E:E').api.ColumnWidth = 40  # Event column\n",
    "        sht.range('F:I').api.ColumnWidth = 12  # Numeric columns\n",
    "        \n",
    "        # Apply conditional formatting to time cells\n",
    "        for idx, row in df.iterrows():\n",
    "            time_cell = sht.range(f'C{71 + idx}')\n",
    "            time_value = row['Time']\n",
    "            \n",
    "            # Highlight early morning times (market opening hours)\n",
    "            if pd.notna(time_value) and \"AM\" in str(time_value):\n",
    "                try:\n",
    "                    hour = int(str(time_value).split(':')[0])\n",
    "                    if hour < 3 or (7 <= hour <= 9):\n",
    "                        time_cell.api.Interior.Color = 0x5C1F1F  # Dark red background\n",
    "                        time_cell.api.Font.Color = 0xFFFFFF  # White text\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Save the workbook\n",
    "        book.save()\n",
    "        print(f\"\\nData successfully exported to Richspread.xlsx at cell B70\")\n",
    "        print(f\"Total rows exported: {len(df)}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "        if driver:\n",
    "            driver.save_screenshot('error_screenshot.png')\n",
    "        return None\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Economic Calendar Scraper\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Run the scraper and get the DataFrame\n",
    "    df = scrape_economic_calendar()\n",
    "    \n",
    "    # Provide feedback on the results\n",
    "    if df is not None:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Economic calendar data successfully scraped!\")\n",
    "        print(f\"DataFrame contains {len(df)} events\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Display basic statistics if running interactively\n",
    "        try:\n",
    "            # These will only work in Jupyter/IPython environments\n",
    "            display(df.head())\n",
    "            display(df.describe())\n",
    "        except NameError:\n",
    "            # Fallback for regular Python execution\n",
    "            print(\"\\nFirst 5 rows:\")\n",
    "            print(df.head())\n",
    "            print(\"\\nNumeric summary:\")\n",
    "            print(df.describe())\n",
    "    else:\n",
    "        print(\"No data was returned. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d9238",
   "metadata": {},
   "source": [
    "Simfa Bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e810df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFMA Corporate Bonds Statistics Scraper\n",
      "=============================================\n",
      "Loading SIFMA page: https://www.sifma.org/resources/research/statistics/us-corporate-bonds-statistics/\n",
      "Waiting for page content to load...\n",
      "\n",
      "Extracting corporate bond statistics...\n",
      "Primary extraction method failed: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010056755c cxxbridge1$str$ptr + 2731064\n",
      "1   chromedriver                        0x000000010055f454 cxxbridge1$str$ptr + 2698032\n",
      "2   chromedriver                        0x00000001000ae3f8 cxxbridge1$string$len + 90664\n",
      "3   chromedriver                        0x00000001000f571c cxxbridge1$string$len + 382284\n",
      "4   chromedriver                        0x0000000100136b1c cxxbridge1$string$len + 649548\n",
      "5   chromedriver                        0x00000001000e9a0c cxxbridge1$string$len + 333884\n",
      "6   chromedriver                        0x000000010052a5e0 cxxbridge1$str$ptr + 2481340\n",
      "7   chromedriver                        0x000000010052d848 cxxbridge1$str$ptr + 2494244\n",
      "8   chromedriver                        0x000000010050b234 cxxbridge1$str$ptr + 2353424\n",
      "9   chromedriver                        0x000000010052e104 cxxbridge1$str$ptr + 2496480\n",
      "10  chromedriver                        0x00000001004fc2e4 cxxbridge1$str$ptr + 2292160\n",
      "11  chromedriver                        0x000000010054dff8 cxxbridge1$str$ptr + 2627284\n",
      "12  chromedriver                        0x000000010054e184 cxxbridge1$str$ptr + 2627680\n",
      "13  chromedriver                        0x000000010055f090 cxxbridge1$str$ptr + 2697068\n",
      "14  libsystem_pthread.dylib             0x0000000190866c0c _pthread_start + 136\n",
      "15  libsystem_pthread.dylib             0x0000000190861b80 thread_start + 8\n",
      "\n",
      "Attempting alternative extraction method...\n",
      "Found Issuance via alternative method: Issuance (as of end-June) $1,172.5 billion, +5.2% Y/Y\n",
      "Found Trading via alternative method: Trading (as of end-June) $60.4 billion ADV, +14.6% Y/Y\n",
      "Found Outstanding via alternative method: Outstanding (as of 1Q25) $11.4 trillion, +3.7% Y/Y\n",
      "\n",
      "Exporting data to Excel...\n",
      "Successfully exported SIFMA data to Excel starting at $B$100\n",
      "Browser session closed\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import xlwings as xw\n",
    "\n",
    "def scrape_sifma_corporate_bonds():\n",
    "    \"\"\"\n",
    "    Scrapes SIFMA corporate bond statistics including year-to-date issuance,\n",
    "    trading volumes, and outstanding amounts. Uses both exact XPaths and\n",
    "    fallback methods for robust data extraction.\n",
    "    \"\"\"\n",
    "    url = \"https://www.sifma.org/resources/research/statistics/us-corporate-bonds-statistics/\"\n",
    "    \n",
    "    # Configure Chrome for headless operation with anti-detection\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading SIFMA page: {url}\")\n",
    "        driver.get(url)\n",
    "        print(\"Waiting for page content to load...\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Extract the main statistics using precise XPath targeting\n",
    "        print(\"\\nExtracting corporate bond statistics...\")\n",
    "        summary_stats = extract_summary_statistics_exact(driver)\n",
    "        \n",
    "        # Export the data to Excel with formatting\n",
    "        print(\"\\nExporting data to Excel...\")\n",
    "        write_to_excel(summary_stats)\n",
    "        \n",
    "        return {\n",
    "            'summary_statistics': summary_stats\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Browser session closed\")\n",
    "        return None\n",
    "\n",
    "def extract_summary_statistics_exact(driver):\n",
    "    \"\"\"\n",
    "    Extracts year-to-date summary statistics using precise XPath selectors.\n",
    "    Falls back to alternative methods if the primary approach fails.\n",
    "    \"\"\"\n",
    "    summary_stats = []\n",
    "    \n",
    "    try:\n",
    "        # Define exact XPaths for the statistics we want\n",
    "        ytd_header_xpath = \"/html/body/div[2]/section/div[1]/article/section/div/p[2]\"\n",
    "        issuance_xpath = \"/html/body/div[2]/section/div[1]/article/section/div/ul[1]/li[1]\"\n",
    "        trading_xpath = \"/html/body/div[2]/section/div[1]/article/section/div/ul[1]/li[2]\"\n",
    "        outstanding_xpath = \"/html/body/div[2]/section/div[1]/article/section/div/ul[1]/li[3]\"\n",
    "        \n",
    "        # Wait for the content to be available\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, ytd_header_xpath)))\n",
    "        \n",
    "        # Get the year-to-date header information\n",
    "        ytd_header = driver.find_element(By.XPATH, ytd_header_xpath)\n",
    "        ytd_text = ytd_header.text.strip()\n",
    "        print(f\"YTD Header: {ytd_text}\")\n",
    "        \n",
    "        # Extract each type of statistic\n",
    "        xpaths = [issuance_xpath, trading_xpath, outstanding_xpath]\n",
    "        types = [\"Issuance\", \"Trading\", \"Outstanding\"]\n",
    "        \n",
    "        for i, xpath in enumerate(xpaths):\n",
    "            try:\n",
    "                element = driver.find_element(By.XPATH, xpath)\n",
    "                stat_text = element.text.strip()\n",
    "                print(f\"Found {types[i]}: {stat_text}\")\n",
    "                \n",
    "                # Parse the raw text into structured data\n",
    "                stat_dict = parse_statistic(stat_text)\n",
    "                stat_dict['type'] = types[i]\n",
    "                summary_stats.append(stat_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting {types[i]}: {e}\")\n",
    "                # Add a placeholder entry so we don't lose track of missing data\n",
    "                summary_stats.append({\n",
    "                    'type': types[i],\n",
    "                    'raw_text': f\"Could not extract {types[i]} data\",\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        return summary_stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Primary extraction method failed: {e}\")\n",
    "        \n",
    "        # Try a more flexible approach using content-based searching\n",
    "        try:\n",
    "            print(\"Attempting alternative extraction method...\")\n",
    "            \n",
    "            # Search for list items containing our target keywords\n",
    "            issuance_items = driver.find_elements(By.XPATH, \"//li[contains(text(), 'Issuance')]\")\n",
    "            trading_items = driver.find_elements(By.XPATH, \"//li[contains(text(), 'Trading')]\")\n",
    "            outstanding_items = driver.find_elements(By.XPATH, \"//li[contains(text(), 'Outstanding')]\")\n",
    "            \n",
    "            items = [\n",
    "                (issuance_items, \"Issuance\"),\n",
    "                (trading_items, \"Trading\"),\n",
    "                (outstanding_items, \"Outstanding\")\n",
    "            ]\n",
    "            \n",
    "            for element_list, type_name in items:\n",
    "                if element_list:\n",
    "                    stat_text = element_list[0].text.strip()\n",
    "                    print(f\"Found {type_name} via alternative method: {stat_text}\")\n",
    "                    \n",
    "                    stat_dict = parse_statistic(stat_text)\n",
    "                    stat_dict['type'] = type_name\n",
    "                    summary_stats.append(stat_dict)\n",
    "                else:\n",
    "                    print(f\"Could not find {type_name} data\")\n",
    "                    summary_stats.append({\n",
    "                        'type': type_name,\n",
    "                        'raw_text': f\"Could not extract {type_name} data\"\n",
    "                    })\n",
    "            \n",
    "            return summary_stats\n",
    "        except Exception as e:\n",
    "            print(f\"Alternative extraction method also failed: {e}\")\n",
    "            return []\n",
    "\n",
    "def parse_statistic(stat_text):\n",
    "    \"\"\"\n",
    "    Parses a raw statistic text string into structured components.\n",
    "    Extracts dollar values, time periods, and percentage changes.\n",
    "    \"\"\"\n",
    "    stat_dict = {'raw_text': stat_text}\n",
    "    \n",
    "    # Extract dollar amounts (e.g., \"$1.23 trillion\")\n",
    "    dollar_match = re.search(r'\\$([0-9,.]+)\\s+(billion|trillion)', stat_text)\n",
    "    if dollar_match:\n",
    "        value = float(dollar_match.group(1).replace(',', ''))\n",
    "        unit = dollar_match.group(2)\n",
    "        stat_dict['value'] = value\n",
    "        stat_dict['unit'] = unit\n",
    "    \n",
    "    # Extract time periods (e.g., \"as of December 2024\")\n",
    "    period_match = re.search(r'\\(as of ([^)]+)\\)', stat_text)\n",
    "    if period_match:\n",
    "        stat_dict['period'] = period_match.group(1)\n",
    "    \n",
    "    # Extract percentage changes (e.g., \"+5.2%\" or \"-3.1%\")\n",
    "    change_match = re.search(r'([+-][0-9.]+%)', stat_text)\n",
    "    if change_match:\n",
    "        stat_dict['change'] = change_match.group(1)\n",
    "    \n",
    "    return stat_dict\n",
    "\n",
    "def write_to_excel(summary_stats):\n",
    "    \"\"\"\n",
    "    Writes the extracted SIFMA data to Excel with professional formatting.\n",
    "    Creates a clean, readable layout starting at cell B100.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the active Excel workbook or create a new one\n",
    "        try:\n",
    "            wb = xw.books.active\n",
    "        except:\n",
    "            wb = xw.Book()\n",
    "        \n",
    "        # Try to use the MarketSnapshot sheet, fallback to active sheet\n",
    "        try:\n",
    "            sheet = wb.sheets['MarketSnapshot']\n",
    "        except:\n",
    "            sheet = wb.sheets.active\n",
    "            print(f\"Using active sheet: {sheet.name}\")\n",
    "        \n",
    "        # Start writing data at cell B100\n",
    "        start_cell = sheet.range('B100')\n",
    "        current_row = start_cell.row\n",
    "        current_col = start_cell.column\n",
    "        \n",
    "        # Create a main header for the section\n",
    "        header_cell = sheet.cells(current_row, current_col)\n",
    "        header_cell.value = \"SIFMA U.S. Corporate Bond Statistics\"\n",
    "        header_cell.font.bold = True\n",
    "        header_cell.font.size = 14\n",
    "        current_row += 2\n",
    "        \n",
    "        # Write the summary statistics section\n",
    "        if summary_stats:\n",
    "            stats_header = sheet.cells(current_row, current_col)\n",
    "            stats_header.value = \"YTD Statistics Summary:\"\n",
    "            stats_header.font.bold = True\n",
    "            current_row += 1\n",
    "            \n",
    "            # Write each statistic on its own row\n",
    "            for stat in summary_stats:\n",
    "                # Use the raw text if available, otherwise build from components\n",
    "                if 'raw_text' in stat:\n",
    "                    sheet.cells(current_row, current_col).value = stat['raw_text']\n",
    "                elif 'type' in stat:\n",
    "                    # Construct a readable string from the parsed components\n",
    "                    text_parts = []\n",
    "                    text_parts.append(stat['type'])\n",
    "                    \n",
    "                    if 'period' in stat:\n",
    "                        text_parts.append(f\"(as of {stat['period']})\")\n",
    "                    \n",
    "                    if 'value' in stat and 'unit' in stat:\n",
    "                        text_parts.append(f\"${stat['value']:,.1f} {stat['unit']}\")\n",
    "                    \n",
    "                    if 'change' in stat:\n",
    "                        text_parts.append(f\"{stat['change']} Y/Y\")\n",
    "                    \n",
    "                    sheet.cells(current_row, current_col).value = \" \".join(text_parts)\n",
    "                \n",
    "                current_row += 1\n",
    "        \n",
    "        print(f\"Successfully exported SIFMA data to Excel starting at {start_cell.address}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to Excel: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"SIFMA Corporate Bonds Statistics Scraper\")\n",
    "    print(\"=\" * 45)\n",
    "    scrape_sifma_corporate_bonds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b2dbf",
   "metadata": {},
   "source": [
    "Sim Market Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734911f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFMA Market Metrics and Musings Scraper\n",
      "=============================================\n",
      "Loading SIFMA insights page: https://www.sifma.org/resources/research/insights/insights-market-metrics-and-trends/\n",
      "Waiting for page content to load...\n",
      "\n",
      "Extracting Market Metrics data...\n",
      "Found Market Metrics section (562 characters)\n",
      "- Found VIX data\n",
      "- Found S&P 500 data\n",
      "- Found Performance data\n",
      "- Found Equity ADV data\n",
      "- Found Options ADV data\n",
      "Total Market Metrics items extracted: 5\n",
      "\n",
      "Extracting Market Musings data...\n",
      "Found Market Musings section (706 characters)\n",
      "Total Market Musings items extracted: 7\n",
      "\n",
      "Exporting data to Excel...\n",
      "Error writing to Excel: name 'xlwings' is not defined\n",
      "Browser session closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rx/whpr72693f3cgk151m8h7fh80000gn/T/ipykernel_93503/2198042961.py\", line 322, in write_to_excel_improved\n",
      "    column_letter = xlwings.utils.col_name(current_col)\n",
      "                    ^^^^^^^\n",
      "NameError: name 'xlwings' is not defined\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import xlwings as xw\n",
    "\n",
    "def scrape_market_metrics(url, excel_start_cell=\"B110\"):\n",
    "    \"\"\"\n",
    "    Scrapes market metrics and musings from SIFMA's insights page.\n",
    "    Extracts key financial indicators like VIX, S&P 500, trading volumes,\n",
    "    and market commentary. Formats the data for clean Excel output.\n",
    "    \n",
    "    Parameters:\n",
    "    url (str): The URL to scrape market data from\n",
    "    excel_start_cell (str): Excel cell to start writing data (default: B110)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure Chrome for headless operation with realistic browser behavior\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading SIFMA insights page: {url}\")\n",
    "        driver.get(url)\n",
    "        print(\"Waiting for page content to load...\")\n",
    "        \n",
    "        # Wait for the page body to be present\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        time.sleep(3)  # Give dynamic content time to load\n",
    "        \n",
    "        # Get all page text for parsing\n",
    "        page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        \n",
    "        # Extract the two main sections we're interested in\n",
    "        print(\"\\nExtracting Market Metrics data...\")\n",
    "        metrics_data = extract_market_metrics_from_text(page_text)\n",
    "        \n",
    "        print(\"\\nExtracting Market Musings data...\")\n",
    "        musings_data = extract_market_musings_from_text(page_text)\n",
    "        \n",
    "        # Export everything to Excel with proper formatting\n",
    "        print(\"\\nExporting data to Excel...\")\n",
    "        write_to_excel_improved(metrics_data, musings_data, excel_start_cell)\n",
    "        \n",
    "        return {\n",
    "            'market_metrics': metrics_data,\n",
    "            'market_musings': musings_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Browser session closed\")\n",
    "\n",
    "def extract_market_metrics_from_text(page_text):\n",
    "    \"\"\"\n",
    "    Extracts structured market metrics data from the page text.\n",
    "    Looks for key indicators like VIX, S&P 500, trading volumes, and performance data.\n",
    "    \"\"\"\n",
    "    metrics_data = []\n",
    "    \n",
    "    # Find where the Market Metrics section starts and ends\n",
    "    metrics_start = page_text.find(\"Market Metrics\")\n",
    "    metrics_end = page_text.find(\"Market Musings\")\n",
    "    \n",
    "    if metrics_start == -1:\n",
    "        print(\"Could not locate Market Metrics section\")\n",
    "        return metrics_data\n",
    "    \n",
    "    # Extract just the metrics section text\n",
    "    if metrics_end > metrics_start:\n",
    "        metrics_text = page_text[metrics_start:metrics_end]\n",
    "    else:\n",
    "        metrics_text = page_text[metrics_start:metrics_start+2000]  # Fallback: take next 2000 chars\n",
    "    \n",
    "    print(f\"Found Market Metrics section ({len(metrics_text)} characters)\")\n",
    "    \n",
    "    # Use regex patterns to extract specific metrics\n",
    "    \n",
    "    # 1. Volatility Index (VIX) data\n",
    "    vix_pattern = r\"Volatility\\s*\\(VIX\\)[:\\s]*Monthly\\s+average\\s+([0-9.]+);\\s*([+-]?[0-9.]+%)\\s*M/M,\\s*([+-]?[0-9.]+%)\\s*Y/Y\"\n",
    "    vix_match = re.search(vix_pattern, metrics_text, re.IGNORECASE)\n",
    "    if vix_match:\n",
    "        metrics_data.append({\n",
    "            \"text\": f\"Volatility (VIX): Monthly average {vix_match.group(1)}; {vix_match.group(2)} M/M, {vix_match.group(3)} Y/Y\"\n",
    "        })\n",
    "        print(\"- Found VIX data\")\n",
    "    \n",
    "    # 2. S&P 500 price data\n",
    "    sp500_pattern = r\"S&P\\s*500\\s*\\(Price\\)[:\\s]*Monthly\\s+average\\s+([0-9,]+\\.?[0-9]*);?\\s*([+-]?[0-9.]+%)\\s*M/M,\\s*([+-]?[0-9.]+%)\\s*Y/Y\"\n",
    "    sp500_match = re.search(sp500_pattern, metrics_text, re.IGNORECASE)\n",
    "    if sp500_match:\n",
    "        metrics_data.append({\n",
    "            \"text\": f\"S&P 500 (Price): Monthly average {sp500_match.group(1)}; {sp500_match.group(2)} M/M, {sp500_match.group(3)} Y/Y\"\n",
    "        })\n",
    "        print(\"- Found S&P 500 data\")\n",
    "    \n",
    "    # 3. Performance metrics (best/worst performers)\n",
    "    perf_pattern = r\"Performance\\s*\\(month[/]?year\\)[:\\s]*Best\\s*=\\s*([^;,]+?);\\s*worst\\s*=\\s*([^;,]+)\"\n",
    "    perf_match = re.search(perf_pattern, metrics_text, re.IGNORECASE)\n",
    "    if perf_match:\n",
    "        metrics_data.append({\n",
    "            \"text\": f\"Performance (month/year): Best = {perf_match.group(1).strip()}; worst = {perf_match.group(2).strip()}\"\n",
    "        })\n",
    "        print(\"- Found Performance data\")\n",
    "    \n",
    "    # 4. Equity average daily volume\n",
    "    equity_pattern = r\"Equity\\s*ADV[:\\s]*Monthly\\s+average\\s+([0-9.]+)\\s*billion\\s+shares;\\s*([+-]?[0-9.]+%)\\s*M/M,\\s*([+-]?[0-9.]+%)\\s*Y/Y\"\n",
    "    equity_match = re.search(equity_pattern, metrics_text, re.IGNORECASE)\n",
    "    if equity_match:\n",
    "        metrics_data.append({\n",
    "            \"text\": f\"Equity ADV: Monthly average {equity_match.group(1)} billion shares; {equity_match.group(2)} M/M, {equity_match.group(3)} Y/Y\"\n",
    "        })\n",
    "        print(\"- Found Equity ADV data\")\n",
    "    \n",
    "    # 5. Options average daily volume\n",
    "    options_pattern = r\"Options\\s*ADV[:\\s]*Monthly\\s+average\\s+([0-9.]+)\\s*million\\s+contracts;\\s*([+-]?[0-9.]+%)\\s*[M/M,]*\\s*([+-]?[0-9.]+%)\\s*Y/Y\"\n",
    "    options_match = re.search(options_pattern, metrics_text, re.IGNORECASE)\n",
    "    if options_match:\n",
    "        metrics_data.append({\n",
    "            \"text\": f\"Options ADV: Monthly average {options_match.group(1)} million contracts; {options_match.group(2)} M/M, {options_match.group(3)} Y/Y\"\n",
    "        })\n",
    "        print(\"- Found Options ADV data\")\n",
    "    \n",
    "    # 6. Market resilience commentary\n",
    "    if \"Markets continue to show resilience\" in metrics_text:\n",
    "        resilience_pattern = r\"Markets continue to show resilience[^.]+\\.\"\n",
    "        resilience_match = re.search(resilience_pattern, metrics_text)\n",
    "        if resilience_match:\n",
    "            metrics_data.append({\n",
    "                \"text\": resilience_match.group(0).strip()\n",
    "            })\n",
    "            print(\"- Found market resilience statement\")\n",
    "    \n",
    "    # 7. Volatility commentary\n",
    "    muted_pattern = r\"Volatility was muted[^.]+\\.\"\n",
    "    muted_match = re.search(muted_pattern, metrics_text)\n",
    "    if muted_match:\n",
    "        metrics_data.append({\n",
    "            \"text\": muted_match.group(0).strip()\n",
    "        })\n",
    "        print(\"- Found volatility commentary\")\n",
    "    \n",
    "    print(f\"Total Market Metrics items extracted: {len(metrics_data)}\")\n",
    "    return metrics_data\n",
    "\n",
    "def extract_market_musings_from_text(page_text):\n",
    "    \"\"\"\n",
    "    Extracts market musings commentary from the page text.\n",
    "    Breaks down the content into readable sentences for better Excel formatting.\n",
    "    \"\"\"\n",
    "    musings_data = []\n",
    "    \n",
    "    # Find the Market Musings section boundaries\n",
    "    musings_start = page_text.find(\"Market Musings\")\n",
    "    \n",
    "    if musings_start == -1:\n",
    "        print(\"Could not locate Market Musings section\")\n",
    "        return musings_data\n",
    "    \n",
    "    # Look for common section endings to determine where musings end\n",
    "    possible_end_markers = [\"Download Full Report\", \"Featured Charts\", \"Additional Resources\", \"Contact\", \"Footer\"]\n",
    "    musings_end = len(page_text)\n",
    "    \n",
    "    for marker in possible_end_markers:\n",
    "        marker_pos = page_text.find(marker, musings_start)\n",
    "        if marker_pos > musings_start and marker_pos < musings_end:\n",
    "            musings_end = marker_pos\n",
    "    \n",
    "    # Extract the musings text\n",
    "    musings_text = page_text[musings_start:musings_end]\n",
    "    \n",
    "    print(f\"Found Market Musings section ({len(musings_text)} characters)\")\n",
    "    \n",
    "    # Clean up by removing the section header\n",
    "    musings_text = musings_text.replace(\"Market Musings\", \"\", 1).strip()\n",
    "    \n",
    "    # Split into sentences for better readability in Excel\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', musings_text)\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        \n",
    "        # Skip very short or empty sentences\n",
    "        if len(sentence) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Skip navigation or non-content elements\n",
    "        if any(skip_word in sentence for skip_word in [\"Download\", \"Chart\", \"Report\", \"Contact\", \"Footer\", \"Navigation\"]):\n",
    "            continue\n",
    "        \n",
    "        # Add as a separate item for clean Excel formatting\n",
    "        musings_data.append({\n",
    "            \"text\": sentence\n",
    "        })\n",
    "    \n",
    "    # If no sentences were found, try extracting key phrases\n",
    "    if not musings_data:\n",
    "        print(\"Trying alternative extraction method for Market Musings...\")\n",
    "        \n",
    "        key_phrases = [\n",
    "            \"continue to show resilience\",\n",
    "            \"posting a record\",\n",
    "            \"Volatility was muted\",\n",
    "            \"Trade policy concerns\",\n",
    "            \"Price level indicators\",\n",
    "            \"inflationary worries\",\n",
    "            \"CPI and Core CPI\",\n",
    "            \"USD/EUR\"\n",
    "        ]\n",
    "        \n",
    "        for phrase in key_phrases:\n",
    "            if phrase in musings_text:\n",
    "                # Extract the sentence containing this phrase\n",
    "                start = musings_text.find(phrase)\n",
    "                sentence_start = max(0, musings_text.rfind('.', 0, start) + 1)\n",
    "                sentence_end = musings_text.find('.', start)\n",
    "                if sentence_end == -1:\n",
    "                    sentence_end = start + 100  # Take next 100 chars if no period found\n",
    "                \n",
    "                sentence = musings_text[sentence_start:sentence_end + 1].strip()\n",
    "                if sentence and len(sentence) > 10:\n",
    "                    musings_data.append({\n",
    "                        \"text\": sentence\n",
    "                    })\n",
    "    \n",
    "    print(f\"Total Market Musings items extracted: {len(musings_data)}\")\n",
    "    return musings_data\n",
    "\n",
    "def write_to_excel_improved(metrics_data, musings_data, start_cell):\n",
    "    \"\"\"\n",
    "    Writes the extracted market data to Excel with professional formatting.\n",
    "    Creates separate sections for metrics and musings with proper styling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the active Excel workbook or create new one\n",
    "        try:\n",
    "            wb = xw.books.active\n",
    "        except:\n",
    "            wb = xw.Book()\n",
    "        \n",
    "        # Try to use MarketSnapshot sheet, fallback to active sheet\n",
    "        try:\n",
    "            sheet = wb.sheets['MarketSnapshot']\n",
    "        except:\n",
    "            sheet = wb.sheets.active\n",
    "            print(f\"Using active sheet: {sheet.name}\")\n",
    "        \n",
    "        # Set starting position\n",
    "        start_range = sheet.range(start_cell)\n",
    "        current_row = start_range.row\n",
    "        current_col = start_range.column\n",
    "        \n",
    "        # Write Market Metrics section header\n",
    "        header_cell = sheet.cells(current_row, current_col)\n",
    "        header_cell.value = \"Market Metrics\"\n",
    "        header_cell.font.bold = True\n",
    "        header_cell.font.size = 14\n",
    "        current_row += 2\n",
    "        \n",
    "        # Write Market Metrics data\n",
    "        if metrics_data:\n",
    "            for metric in metrics_data:\n",
    "                cell = sheet.cells(current_row, current_col)\n",
    "                cell.value = metric.get(\"text\", \"\")\n",
    "                cell.api.WrapText = True  # Enable text wrapping for long content\n",
    "                current_row += 1\n",
    "        else:\n",
    "            sheet.cells(current_row, current_col).value = \"No Market Metrics data found\"\n",
    "            current_row += 1\n",
    "        \n",
    "        current_row += 2\n",
    "        \n",
    "        # Write Market Musings section header\n",
    "        header_cell = sheet.cells(current_row, current_col)\n",
    "        header_cell.value = \"Market Musings\"\n",
    "        header_cell.font.bold = True\n",
    "        header_cell.font.size = 14\n",
    "        current_row += 2\n",
    "        \n",
    "        # Write Market Musings data with bullet points\n",
    "        if musings_data:\n",
    "            for musing in musings_data:\n",
    "                cell = sheet.cells(current_row, current_col)\n",
    "                text = musing.get(\"text\", \"\")\n",
    "                \n",
    "                # Add bullet point for visual organization\n",
    "                if text and not text.startswith(\"â€¢\"):\n",
    "                    text = f\"â€¢ {text}\"\n",
    "                \n",
    "                cell.value = text\n",
    "                cell.api.WrapText = True\n",
    "                \n",
    "                # Auto-fit row height for content\n",
    "                sheet.range(f'{current_row}:{current_row}').row_height = None\n",
    "                \n",
    "                current_row += 1\n",
    "        else:\n",
    "            sheet.cells(current_row, current_col).value = \"No Market Musings data found\"\n",
    "        \n",
    "        # Set column width for better readability\n",
    "        column_letter = xlwings.utils.col_name(current_col)\n",
    "        sheet.range(f'{column_letter}:{column_letter}').column_width = 80\n",
    "        \n",
    "        print(f\"Successfully exported market data to Excel starting at {start_cell}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to Excel: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"SIFMA Market Metrics and Musings Scraper\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Target URL for SIFMA market insights\n",
    "    url = \"https://www.sifma.org/resources/research/insights/insights-market-metrics-and-trends/\"\n",
    "    scrape_market_metrics(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5f461",
   "metadata": {},
   "source": [
    "Sim Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8072c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFMA US Economic Survey Scraper\n",
      "===================================\n",
      "Loading SIFMA Economic Survey page: https://www.sifma.org/resources/research/economics/us-economic-survey/\n",
      "Waiting for page content to load...\n",
      "Found survey date: This survey was populated between\n",
      "\n",
      "Extracting Key Takeaways...\n",
      "Trying keyword-based extraction for Key Takeaways...\n",
      "Found with keyword 'Monetary Policy:': Monetary Policy: 75% of our economists expect one or more rate cuts by the end of 2025 for a total decrease of roughly 50bps. The median forecaster looks for the midpoint of the target range to end 2025 at 3.926% (roughly 50 bps in cuts from current rate) and to end 2026 at 3.625% (a total of 75 bps in cuts from the current rate). Nearly 60% of our economists estimate the neutral nominal fed funds rate to be 3.0%-3.5%.\n",
      "Found with keyword 'Inflation:': Inflation: The median forecaster looks for core PCE inflation to end 2025 at 3.1% (year-over-year), up 0.7 pps from the last full survey in November 2024 and 0.3 pps higher than the March 2025 flash poll. The top factors influencing forecasts for core inflation estimates are trade policy, inflation expectations, and growth in domestic demand.\n",
      "Found with keyword 'Economy:': Economy: The median economist forecasts real GDP will grow 0.9% in 2025; -1.0 pps from our last full survey in November 2024 and -0.6 pps from the March 2025 flash poll. Over 70% of our economists put the probability of recession from 30% to 50%. The top factors impacting US economic growth are US trade policy, US labor market developments, and US monetary policy. US trade policy also shows up near the top in both upside and downside risks to the economy.\n",
      "\n",
      "Exporting data to Excel...\n",
      "Successfully exported US Economic Survey data to Excel starting at B135\n",
      "Browser session closed\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import xlwings as xw\n",
    "\n",
    "def scrape_economic_survey(excel_start_cell=\"B135\"):\n",
    "    \"\"\"\n",
    "    Scrapes SIFMA's US Economic Survey Key Takeaways using multiple extraction methods.\n",
    "    Extracts survey dates and key economic insights including monetary policy,\n",
    "    inflation forecasts, and economic growth projections.\n",
    "    \n",
    "    Parameters:\n",
    "    excel_start_cell (str): Excel cell to start writing data (default: B135)\n",
    "    \"\"\"\n",
    "    url = \"https://www.sifma.org/resources/research/economics/us-economic-survey/\"\n",
    "    \n",
    "    # Configure Chrome for headless operation with anti-detection\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading SIFMA Economic Survey page: {url}\")\n",
    "        driver.get(url)\n",
    "        print(\"Waiting for page content to load...\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Extract when the survey was conducted\n",
    "        survey_date = extract_survey_date(driver)\n",
    "        \n",
    "        # Extract the main insights from the Key Takeaways section\n",
    "        print(\"\\nExtracting Key Takeaways...\")\n",
    "        takeaways = extract_key_takeaways(driver)\n",
    "        \n",
    "        # Export everything to Excel with proper formatting\n",
    "        print(\"\\nExporting data to Excel...\")\n",
    "        write_to_excel(survey_date, takeaways, excel_start_cell)\n",
    "        \n",
    "        return {\n",
    "            'survey_date': survey_date,\n",
    "            'key_takeaways': takeaways\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Browser session closed\")\n",
    "        return None\n",
    "\n",
    "def extract_survey_date(driver):\n",
    "    \"\"\"\n",
    "    Extracts the survey date information using multiple approaches.\n",
    "    Looks for text about when the survey was populated.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Approach 1: Look for paragraph containing \"survey was populated\"\n",
    "        survey_date_elements = driver.find_elements(By.XPATH, \"//p[contains(text(), 'survey was populated')]\")\n",
    "        \n",
    "        # Approach 2: Broader search for any element containing the phrase\n",
    "        if not survey_date_elements:\n",
    "            survey_date_elements = driver.find_elements(By.XPATH, \"//*[contains(text(), 'survey was populated')]\")\n",
    "        \n",
    "        # Approach 3: Look for date info near the main title\n",
    "        if not survey_date_elements:\n",
    "            title_elements = driver.find_elements(By.XPATH, \"//h1[contains(text(), 'US Economic Survey')] | //h2[contains(text(), 'US Economic Survey')]\")\n",
    "            if title_elements:\n",
    "                title = title_elements[0]\n",
    "                survey_date_elements = driver.find_elements(By.XPATH, f\"following-sibling::*[1]\")\n",
    "        \n",
    "        if survey_date_elements:\n",
    "            survey_date_text = survey_date_elements[0].text.strip()\n",
    "            print(f\"Found survey date: {survey_date_text}\")\n",
    "            return survey_date_text\n",
    "        \n",
    "        # Fallback: Extract from entire page content using regex\n",
    "        page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        date_match = re.search(r\"survey was populated between ([A-Za-z]+ \\d+) and ([A-Za-z]+ \\d+, \\d{4})\", page_text)\n",
    "        if date_match:\n",
    "            survey_date_text = f\"This survey was populated between {date_match.group(1)} and {date_match.group(2)}.\"\n",
    "            print(f\"Found survey date from page text: {survey_date_text}\")\n",
    "            return survey_date_text\n",
    "            \n",
    "        return \"Survey date not found\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting survey date: {e}\")\n",
    "        return \"Error extracting survey date\"\n",
    "\n",
    "def extract_key_takeaways(driver):\n",
    "    \"\"\"\n",
    "    Extracts Key Takeaways section using multiple extraction strategies.\n",
    "    Tries heading-based extraction first, then falls back to keyword searches.\n",
    "    \"\"\"\n",
    "    takeaways = []\n",
    "    \n",
    "    try:\n",
    "        # Primary method: Find the Key Takeaways heading and extract list items\n",
    "        heading_found = False\n",
    "        heading_elements = driver.find_elements(By.XPATH, \n",
    "            \"//h1[contains(text(), 'Key Takeaways')] | \"\n",
    "            \"//h2[contains(text(), 'Key Takeaways')] | \"\n",
    "            \"//h3[contains(text(), 'Key Takeaways')]\"\n",
    "        )\n",
    "        \n",
    "        if heading_elements:\n",
    "            heading = heading_elements[0]\n",
    "            print(\"Found 'Key Takeaways' heading\")\n",
    "            heading_found = True\n",
    "            \n",
    "            # Look for list items that follow the heading\n",
    "            try:\n",
    "                list_items = driver.find_elements(By.XPATH, \n",
    "                    f\"//h1[contains(text(), 'Key Takeaways')]/following::li | \"\n",
    "                    f\"//h2[contains(text(), 'Key Takeaways')]/following::li | \"\n",
    "                    f\"//h3[contains(text(), 'Key Takeaways')]/following::li\"\n",
    "                )\n",
    "                \n",
    "                # Alternative: Look for bullet points in content area\n",
    "                if not list_items:\n",
    "                    list_items = driver.find_elements(By.CSS_SELECTOR, \".entry-content li\")\n",
    "                \n",
    "                print(f\"Found {len(list_items)} key takeaway items\")\n",
    "                \n",
    "                for item in list_items:\n",
    "                    text = item.text.strip()\n",
    "                    if text:\n",
    "                        print(f\"- {text}\")\n",
    "                        parsed_takeaway = parse_takeaway(text)\n",
    "                        takeaways.append(parsed_takeaway)\n",
    "                        \n",
    "                        # Stop if we reach content about \"forecast tables\" (typically the end)\n",
    "                        if \"forecast tables\" in text.lower():\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                print(f\"Error finding list items: {e}\")\n",
    "        \n",
    "        # Fallback method: Search by economic keywords if primary method fails\n",
    "        if not heading_found or not takeaways:\n",
    "            print(\"Trying keyword-based extraction for Key Takeaways...\")\n",
    "            \n",
    "            # Search for common economic survey categories\n",
    "            keywords = [\"Monetary Policy:\", \"Inflation:\", \"Economy:\", \"Growth:\", \"Interest Rates:\"]\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                elements = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{keyword}')]\")\n",
    "                for element in elements:\n",
    "                    # Navigate up the DOM to find the containing list item\n",
    "                    parent = element\n",
    "                    for _ in range(3):  # Check up to 3 parent levels\n",
    "                        try:\n",
    "                            if parent.tag_name == \"li\":\n",
    "                                text = parent.text.strip()\n",
    "                                print(f\"Found with keyword '{keyword}': {text}\")\n",
    "                                parsed_takeaway = parse_takeaway(text)\n",
    "                                \n",
    "                                # Avoid duplicates by checking category\n",
    "                                is_duplicate = False\n",
    "                                for existing in takeaways:\n",
    "                                    if existing.get('category') == parsed_takeaway.get('category'):\n",
    "                                        is_duplicate = True\n",
    "                                        break\n",
    "                                        \n",
    "                                if not is_duplicate:\n",
    "                                    takeaways.append(parsed_takeaway)\n",
    "                                break\n",
    "                            parent = parent.find_element(By.XPATH, \"..\")\n",
    "                        except:\n",
    "                            break\n",
    "        \n",
    "        # Last resort: Extract from page text using pattern matching\n",
    "        if not takeaways:\n",
    "            print(\"Trying pattern-based extraction from page text...\")\n",
    "            page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            \n",
    "            # Look for content related to key economic categories\n",
    "            categories = [\"Monetary Policy\", \"Inflation\", \"Economy\", \"Growth\", \"Interest Rates\"]\n",
    "            \n",
    "            for category in categories:\n",
    "                # Use regex to find content sections for each category\n",
    "                pattern = f\"{category}[:\\s]+(.*?)(?=(?:{\"|\".join(categories)})[:\\s]+|\\Z)\"\n",
    "                matches = re.findall(pattern, page_text, re.DOTALL | re.IGNORECASE)\n",
    "                \n",
    "                for match in matches:\n",
    "                    text = match.strip()\n",
    "                    if text:\n",
    "                        print(f\"Found {category} content from page text\")\n",
    "                        takeaways.append({\n",
    "                            'category': category,\n",
    "                            'content': text,\n",
    "                            'raw_text': f\"{category}: {text}\"\n",
    "                        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting Key Takeaways: {e}\")\n",
    "    \n",
    "    return takeaways\n",
    "\n",
    "def parse_takeaway(text):\n",
    "    \"\"\"\n",
    "    Parses a key takeaway text into structured components.\n",
    "    Extracts categories and specific data points like percentages and forecasts.\n",
    "    \"\"\"\n",
    "    takeaway = {'raw_text': text}\n",
    "    \n",
    "    # Try to split into category and content using colon separator\n",
    "    category_match = re.match(r'^([^:]+):\\s*(.*)', text, re.DOTALL)\n",
    "    \n",
    "    if category_match:\n",
    "        category = category_match.group(1).strip()\n",
    "        content = category_match.group(2).strip()\n",
    "        takeaway['category'] = category\n",
    "        takeaway['content'] = content\n",
    "        \n",
    "        # Extract specific data for Monetary Policy content\n",
    "        if \"Monetary Policy\" in category:\n",
    "            # Extract percentage of economists expecting rate cuts\n",
    "            rate_cut_match = re.search(r'(\\d+)%\\s+of our economists expect', content)\n",
    "            if rate_cut_match:\n",
    "                takeaway['rate_cut_percentage'] = rate_cut_match.group(1)\n",
    "            \n",
    "            # Extract target rate forecasts for 2025\n",
    "            target_2025_match = re.search(r'end 2025 at (\\d+\\.\\d+)%', content)\n",
    "            if target_2025_match:\n",
    "                takeaway['target_rate_2025'] = target_2025_match.group(1)\n",
    "                \n",
    "            # Extract target rate forecasts for 2026\n",
    "            target_2026_match = re.search(r'end 2026 at (\\d+\\.\\d+)%', content)\n",
    "            if target_2026_match:\n",
    "                takeaway['target_rate_2026'] = target_2026_match.group(1)\n",
    "                \n",
    "            # Extract neutral rate range estimates\n",
    "            neutral_rate_match = re.search(r'neutral nominal fed funds rate to be (\\d+\\.\\d+%-\\d+\\.\\d+%)', content)\n",
    "            if neutral_rate_match:\n",
    "                takeaway['neutral_rate_range'] = neutral_rate_match.group(1)\n",
    "        \n",
    "        # Extract specific data for Inflation content\n",
    "        elif \"Inflation\" in category:\n",
    "            # Extract core PCE forecast\n",
    "            inflation_match = re.search(r'end 2025 at (\\d+\\.\\d+)%', content)\n",
    "            if inflation_match:\n",
    "                takeaway['core_pce_2025'] = inflation_match.group(1)\n",
    "            \n",
    "            # Extract change from previous surveys\n",
    "            change_match = re.search(r'up (\\d+\\.\\d+) pps from the last full survey', content)\n",
    "            if change_match:\n",
    "                takeaway['change_from_previous'] = change_match.group(1)\n",
    "                \n",
    "            # Extract top factors affecting inflation\n",
    "            factors_match = re.search(r'top factors .* are (.*?)\\.', content, re.DOTALL)\n",
    "            if factors_match:\n",
    "                factors = factors_match.group(1).strip()\n",
    "                takeaway['top_factors'] = factors\n",
    "        \n",
    "        # Extract specific data for Economy content\n",
    "        elif \"Economy\" in category:\n",
    "            # Extract GDP growth forecast\n",
    "            gdp_match = re.search(r'grow (\\d+\\.\\d+)% in 2025', content)\n",
    "            if gdp_match:\n",
    "                takeaway['gdp_growth_2025'] = gdp_match.group(1)\n",
    "            \n",
    "            # Extract recession probability data\n",
    "            recession_match = re.search(r'(\\d+)% of our economists put the probability', content)\n",
    "            if recession_match:\n",
    "                takeaway['economists_percentage'] = recession_match.group(1)\n",
    "                \n",
    "            # Extract probability range for recession\n",
    "            prob_range_match = re.search(r'probability of recession from (\\d+)% to (\\d+)%', content)\n",
    "            if prob_range_match:\n",
    "                takeaway['recession_probability_low'] = prob_range_match.group(1)\n",
    "                takeaway['recession_probability_high'] = prob_range_match.group(2)\n",
    "    else:\n",
    "        # If no clear category found, infer from keywords in the text\n",
    "        if \"rate cuts\" in text.lower() or \"monetary policy\" in text.lower():\n",
    "            takeaway['category'] = \"Monetary Policy\"\n",
    "            takeaway['content'] = text\n",
    "        elif \"inflation\" in text.lower() or \"pce\" in text.lower():\n",
    "            takeaway['category'] = \"Inflation\"\n",
    "            takeaway['content'] = text\n",
    "        elif \"gdp\" in text.lower() or \"economy\" in text.lower() or \"recession\" in text.lower():\n",
    "            takeaway['category'] = \"Economy\"\n",
    "            takeaway['content'] = text\n",
    "        else:\n",
    "            takeaway['category'] = \"Other\"\n",
    "            takeaway['content'] = text\n",
    "    \n",
    "    return takeaway\n",
    "\n",
    "def write_to_excel(survey_date, takeaways, start_cell):\n",
    "    \"\"\"\n",
    "    Writes the extracted economic survey data to Excel with professional formatting.\n",
    "    Creates organized sections for survey info and key takeaways.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the active Excel workbook or create new one\n",
    "        try:\n",
    "            wb = xw.books.active\n",
    "        except:\n",
    "            wb = xw.Book()\n",
    "        \n",
    "        # Try to use MarketSnapshot sheet, fallback to active sheet\n",
    "        try:\n",
    "            sheet = wb.sheets['MarketSnapshot']\n",
    "        except:\n",
    "            sheet = wb.sheets.active\n",
    "            print(f\"Using active sheet: {sheet.name}\")\n",
    "        \n",
    "        # Set starting position\n",
    "        start_range = sheet.range(start_cell)\n",
    "        current_row = start_range.row\n",
    "        current_col = start_range.column\n",
    "        \n",
    "        # Write main header for the economic survey section\n",
    "        header_cell = sheet.cells(current_row, current_col)\n",
    "        header_cell.value = \"US Economic Survey\"\n",
    "        header_cell.font.bold = True\n",
    "        header_cell.font.size = 14\n",
    "        current_row += 1\n",
    "        \n",
    "        # Write Key Takeaways section header\n",
    "        takeaways_header = sheet.cells(current_row, current_col)\n",
    "        takeaways_header.value = \"Key Takeaways\"\n",
    "        takeaways_header.font.bold = True\n",
    "        takeaways_header.font.size = 12\n",
    "        current_row += 2\n",
    "        \n",
    "        # Write each key takeaway with proper formatting\n",
    "        if takeaways:\n",
    "            for takeaway in takeaways:\n",
    "                # Write category as bold header\n",
    "                if 'category' in takeaway:\n",
    "                    category_cell = sheet.cells(current_row, current_col)\n",
    "                    category_cell.value = f\"{takeaway['category']}:\"\n",
    "                    category_cell.font.bold = True\n",
    "                    \n",
    "                    # Write content in the adjacent cell\n",
    "                    if 'content' in takeaway:\n",
    "                        content_cell = sheet.cells(current_row, current_col + 1)\n",
    "                        content_cell.value = takeaway['content']\n",
    "                        content_cell.api.WrapText = True  # Enable text wrapping\n",
    "                    \n",
    "                # Fallback: write raw text if structured data isn't available\n",
    "                elif 'raw_text' in takeaway:\n",
    "                    sheet.cells(current_row, current_col).value = takeaway['raw_text']\n",
    "                \n",
    "                current_row += 1\n",
    "                current_row += 1  # Add spacing between categories\n",
    "        else:\n",
    "            sheet.cells(current_row, current_col).value = \"No Key Takeaways found\"\n",
    "        \n",
    "        print(f\"Successfully exported US Economic Survey data to Excel starting at {start_cell}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to Excel: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"SIFMA US Economic Survey Scraper\")\n",
    "    print(\"=\" * 35)\n",
    "    scrape_economic_survey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b4c3b",
   "metadata": {},
   "source": [
    "2y 10y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a99ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fredapi import Fred\n",
    "import xlwings as xw\n",
    "from datetime import datetime\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "def analyze_treasury_yield_curve():\n",
    "    \"\"\"\n",
    "    Retrieves and analyzes the 10-Year minus 2-Year Treasury yield spread from FRED.\n",
    "    Creates summary statistics and charts, then exports everything to Excel.\n",
    "    This indicator is commonly used to assess recession probability and yield curve shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the FRED API - replace with your own API key\n",
    "    api_key = 'YOUR_FRED_API_KEY_HERE'\n",
    "    fred = Fred(api_key=api_key)\n",
    "    \n",
    "    # Retrieve the T10Y2Y data (10-Year Treasury minus 2-Year Treasury spread)\n",
    "    print(\"Retrieving T10Y2Y data from FRED...\")\n",
    "    t10y2y = fred.get_series('T10Y2Y')\n",
    "    \n",
    "    # Display basic information about the data\n",
    "    print(\"\\nT10Y2Y - 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity\")\n",
    "    print(f\"Latest value: {t10y2y.iloc[-1]:.2f}%\")\n",
    "    print(f\"Data range: {t10y2y.index.min()} to {t10y2y.index.max()}\")\n",
    "    print(f\"Total data points: {len(t10y2y)}\")\n",
    "    \n",
    "    try:\n",
    "        # Open the Excel workbook for data export\n",
    "        print(\"\\nOpening Excel workbook...\")\n",
    "        book = xw.Book('Richspread.xlsx')\n",
    "        sheet = book.sheets['MarketSnapshot']\n",
    "        \n",
    "        # Export summary statistics to Excel\n",
    "        export_summary_stats(t10y2y, book, sheet, \"B150\")\n",
    "        \n",
    "        # Create and insert chart into Excel\n",
    "        chart_file = create_and_insert_chart(t10y2y, book, sheet, \"G150\")\n",
    "        \n",
    "        # Save the workbook with our new data\n",
    "        print(\"\\nSaving workbook...\")\n",
    "        book.save()\n",
    "        \n",
    "        print(\"\\nâœ“ Export completed successfully!\")\n",
    "        print(\"- Summary statistics at cell B150\")\n",
    "        print(\"- Chart at cell G150\")\n",
    "        \n",
    "        # Clean up temporary chart file\n",
    "        cleanup_temp_file(chart_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Display the chart in Python for immediate viewing\n",
    "    display_chart_in_python(t10y2y)\n",
    "\n",
    "def export_summary_stats(series_data, book, sheet, stats_cell=\"B150\"):\n",
    "    \"\"\"\n",
    "    Exports summary statistics for the yield curve data to Excel.\n",
    "    Creates a formatted table with key metrics and dates.\n",
    "    \"\"\"\n",
    "    print(\"\\nExporting summary statistics...\")\n",
    "    start_row = sheet.range(stats_cell).row\n",
    "    start_col = sheet.range(stats_cell).column\n",
    "    \n",
    "    # Write section header\n",
    "    current_row = start_row + 2\n",
    "    stats_header = sheet.cells(current_row, start_col)\n",
    "    stats_header.value = \"Summary Statistics\"\n",
    "    stats_header.font.bold = True\n",
    "    \n",
    "    # Create summary data with key metrics\n",
    "    summary_data = [\n",
    "        [\"Latest Value:\", f\"{series_data.iloc[-1]:.2f}%\"],\n",
    "        [\"Date Range:\", f\"{series_data.index.min().strftime('%Y-%m-%d')} to {series_data.index.max().strftime('%Y-%m-%d')}\"],\n",
    "        [\"Average:\", f\"{series_data.mean():.2f}%\"],\n",
    "        [\"Min Value:\", f\"{series_data.min():.2f}% on {series_data.idxmin().strftime('%Y-%m-%d')}\"],\n",
    "        [\"Max Value:\", f\"{series_data.max():.2f}% on {series_data.idxmax().strftime('%Y-%m-%d')}\"]\n",
    "    ]\n",
    "    \n",
    "    # Write each statistic to Excel\n",
    "    for i, (label, value) in enumerate(summary_data):\n",
    "        current_row += 1\n",
    "        sheet.cells(current_row, start_col).value = label\n",
    "        sheet.cells(current_row, start_col + 1).value = value\n",
    "    \n",
    "    # Auto-fit columns for better readability\n",
    "    sheet.range((start_row, start_col), (current_row, start_col + 1)).columns.autofit()\n",
    "    \n",
    "    print(\"Summary statistics exported successfully!\")\n",
    "\n",
    "def create_and_insert_chart(series_data, book, sheet, chart_cell=\"G150\"):\n",
    "    \"\"\"\n",
    "    Creates a professional chart of the yield curve data and inserts it into Excel.\n",
    "    Handles temporary file creation and cleanup automatically.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating chart...\")\n",
    "    \n",
    "    # Create unique filename for temporary chart file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    chart_filename = f\"t10y2y_chart_{timestamp}.png\"\n",
    "    chart_path = os.path.join(tempfile.gettempdir(), chart_filename)\n",
    "    \n",
    "    try:\n",
    "        # Create the matplotlib figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Plot the yield curve data\n",
    "        ax.plot(series_data.index, series_data.values, color='#1f77b4', linewidth=1.5)\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title('10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity',\n",
    "                    fontsize=14, pad=20)\n",
    "        ax.set_ylabel('Percentage Points', fontsize=12)\n",
    "        \n",
    "        # Add horizontal line at zero (important for yield curve analysis)\n",
    "        ax.axhline(y=0, color='red', linestyle='-', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set reasonable y-axis limits\n",
    "        ax.set_ylim(-2.5, 3.5)\n",
    "        \n",
    "        # Format x-axis with 5-year intervals\n",
    "        years = pd.date_range(start='1976', end='2025', freq='5Y')\n",
    "        ax.set_xticks(years)\n",
    "        ax.set_xticklabels([str(year.year) for year in years])\n",
    "        \n",
    "        # Adjust layout and save\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(chart_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f\"Chart saved to: {chart_path}\")\n",
    "        \n",
    "        # Brief pause to ensure file is written\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Verify the file was created successfully\n",
    "        if not os.path.exists(chart_path):\n",
    "            raise FileNotFoundError(f\"Chart file was not created at {chart_path}\")\n",
    "        \n",
    "        # Clear any existing charts in the target area\n",
    "        clear_existing_pictures(sheet, chart_cell)\n",
    "        \n",
    "        # Insert the new chart into Excel\n",
    "        insert_chart_into_excel(sheet, chart_path, chart_cell)\n",
    "        \n",
    "        return chart_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating/inserting chart: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def clear_existing_pictures(sheet, chart_cell):\n",
    "    \"\"\"\n",
    "    Removes any existing pictures in the target chart area to avoid overlap.\n",
    "    \"\"\"\n",
    "    print(\"Clearing existing pictures in target area...\")\n",
    "    target_left = sheet.range(chart_cell).left\n",
    "    target_top = sheet.range(chart_cell).top\n",
    "    \n",
    "    # Create a list copy to avoid modification during iteration\n",
    "    for pic in list(sheet.pictures):\n",
    "        try:\n",
    "            # Check if picture is in our target area\n",
    "            if (pic.left >= target_left - 50 and\n",
    "                pic.left <= target_left + 650 and\n",
    "                pic.top >= target_top - 50 and\n",
    "                pic.top <= target_top + 350):\n",
    "                pic.delete()\n",
    "        except:\n",
    "            pass  # Skip if picture can't be accessed\n",
    "\n",
    "def insert_chart_into_excel(sheet, chart_path, chart_cell):\n",
    "    \"\"\"\n",
    "    Inserts the chart image into Excel using multiple methods for compatibility.\n",
    "    \"\"\"\n",
    "    print(f\"Inserting chart at cell {chart_cell}...\")\n",
    "    target_left = sheet.range(chart_cell).left\n",
    "    target_top = sheet.range(chart_cell).top\n",
    "    \n",
    "    # Method 1: Try standard xlwings approach\n",
    "    try:\n",
    "        pic = sheet.pictures.add(os.path.abspath(chart_path),\n",
    "                               left=target_left,\n",
    "                               top=target_top,\n",
    "                               width=600,\n",
    "                               height=300)\n",
    "        print(\"Chart inserted successfully using standard method!\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Standard method failed: {e}\")\n",
    "    \n",
    "    # Method 2: Try alternative API approach\n",
    "    try:\n",
    "        pic = sheet.api.Pictures.Insert(os.path.abspath(chart_path))\n",
    "        pic.Left = target_left\n",
    "        pic.Top = target_top\n",
    "        pic.Width = 600\n",
    "        pic.Height = 300\n",
    "        print(\"Chart inserted successfully using API method!\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"API method failed: {e}\")\n",
    "        raise Exception(\"Failed to insert chart with both methods\")\n",
    "\n",
    "def cleanup_temp_file(chart_file):\n",
    "    \"\"\"\n",
    "    Safely removes the temporary chart file after Excel processing.\n",
    "    \"\"\"\n",
    "    if chart_file and os.path.exists(chart_file):\n",
    "        try:\n",
    "            # Wait to ensure Excel has finished with the file\n",
    "            time.sleep(2)\n",
    "            os.remove(chart_file)\n",
    "            print(f\"Temporary chart file {chart_file} deleted\")\n",
    "        except:\n",
    "            print(f\"Note: Could not delete {chart_file}. You can delete it manually.\")\n",
    "\n",
    "def display_chart_in_python(series_data):\n",
    "    \"\"\"\n",
    "    Displays the yield curve chart in Python for immediate viewing.\n",
    "    \"\"\"\n",
    "    print(\"\\nDisplaying chart in Python...\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(series_data.index, series_data.values, color=\"#164c72\", linewidth=1.5)\n",
    "    plt.title('10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity')\n",
    "    plt.ylabel('Percentage Points')\n",
    "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"FRED Treasury Yield Curve Analyzer\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"This script analyzes the 10Y-2Y Treasury spread,\")\n",
    "    print(\"an important indicator for recession forecasting.\")\n",
    "    print(\"\\nNote: You'll need to replace 'YOUR_FRED_API_KEY_HERE' with your actual FRED API key.\")\n",
    "    print(\"Get one free at: https://fred.stlouisfed.org/docs/api/api_key.html\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    analyze_treasury_yield_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5acf6af",
   "metadata": {},
   "source": [
    "Treasury rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fredapi import Fred\n",
    "from datetime import datetime, timedelta\n",
    "import xlwings as xw\n",
    "\n",
    "def analyze_treasury_yield_curve():\n",
    "    \"\"\"\n",
    "    Retrieves complete US Treasury yield curve data from FRED across all maturities.\n",
    "    Exports the most recent 5 days of data to Excel and creates a yield curve visualization.\n",
    "    This provides a comprehensive view of interest rates across different time periods.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the FRED API - replace with your own API key\n",
    "    api_key = 'YOUR_FRED_API_KEY_HERE'\n",
    "    fred = Fred(api_key=api_key)\n",
    "    \n",
    "    # Connect to Excel workbook\n",
    "    book = xw.Book('Richspread.xlsx')\n",
    "    sheet = book.sheets['MarketSnapshot']\n",
    "    start_cell = \"B180\"\n",
    "    \n",
    "    # Define all Treasury maturities and their FRED series identifiers\n",
    "    maturity_series = {\n",
    "        '1M': 'DGS1MO',   # 1-Month Treasury\n",
    "        '3M': 'DGS3MO',   # 3-Month Treasury\n",
    "        '6M': 'DGS6MO',   # 6-Month Treasury\n",
    "        '1Y': 'DGS1',     # 1-Year Treasury\n",
    "        '2Y': 'DGS2',     # 2-Year Treasury\n",
    "        '3Y': 'DGS3',     # 3-Year Treasury\n",
    "        '5Y': 'DGS5',     # 5-Year Treasury\n",
    "        '7Y': 'DGS7',     # 7-Year Treasury\n",
    "        '10Y': 'DGS10',   # 10-Year Treasury\n",
    "        '20Y': 'DGS20',   # 20-Year Treasury\n",
    "        '30Y': 'DGS30'    # 30-Year Treasury\n",
    "    }\n",
    "    \n",
    "    # Retrieve yield data for each maturity\n",
    "    treasury_rates = {}\n",
    "    print(\"Retrieving Treasury yield data from FRED...\")\n",
    "    \n",
    "    for label, series_id in maturity_series.items():\n",
    "        try:\n",
    "            treasury_rates[label] = fred.get_series(series_id)\n",
    "            print(f\"Retrieved {label} Treasury rate (Series: {series_id})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not retrieve {label} ({series_id}): {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame for easy manipulation\n",
    "    treasury_rates_df = pd.DataFrame(treasury_rates)\n",
    "    \n",
    "    # Get the most recent 5 days of data (excluding weekends/holidays when markets are closed)\n",
    "    treasury_rates_last5 = treasury_rates_df.tail(5)\n",
    "    \n",
    "    # Display recent data for verification\n",
    "    print(\"\\nRecent Treasury Yield Curve Rates (last 5 days):\")\n",
    "    print(treasury_rates_last5)\n",
    "    \n",
    "    # Export to Excel with proper formatting\n",
    "    sheet.range(start_cell).options(index=True, header=True).value = treasury_rates_last5\n",
    "    print(f\"Treasury yield curve DataFrame (last 5 days) exported to {book.name} at {start_cell}\")\n",
    "    \n",
    "    # Create yield curve visualization\n",
    "    create_yield_curve_chart(treasury_rates_last5, maturity_series)\n",
    "\n",
    "def create_yield_curve_chart(data, maturity_series):\n",
    "    \"\"\"\n",
    "    Creates a visual representation of the current Treasury yield curve.\n",
    "    Shows how interest rates vary across different maturity periods.\n",
    "    \"\"\"\n",
    "    if data.empty:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Get the most recent day's data\n",
    "    most_recent = data.iloc[-1]\n",
    "    current_date = data.index[-1].strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Convert maturity labels to numeric values for proper plotting\n",
    "    # This allows us to show the curve shape accurately\n",
    "    maturities_numeric = []\n",
    "    for mat in maturity_series.keys():\n",
    "        if 'MO' in mat or 'M' in mat:\n",
    "            # Convert months to years (e.g., 3M = 0.25 years)\n",
    "            num = int(''.join(filter(str.isdigit, mat)))\n",
    "            maturities_numeric.append(num/12)\n",
    "        else:\n",
    "            # Convert years to numeric (e.g., 10Y = 10)\n",
    "            maturities_numeric.append(int(''.join(filter(str.isdigit, mat))))\n",
    "    \n",
    "    # Create the yield curve plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(maturities_numeric, most_recent.values, marker='o', linewidth=2, markersize=6)\n",
    "    \n",
    "    # Format the chart\n",
    "    plt.xticks(maturities_numeric, list(maturity_series.keys()))\n",
    "    plt.title(f\"US Treasury Yield Curve on {current_date}\")\n",
    "    plt.xlabel(\"Maturity\")\n",
    "    plt.ylabel(\"Yield (%)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Treasury Yield Curve Analyzer\")\n",
    "    print(\"=\" * 35)\n",
    "    print(\"This script retrieves current Treasury rates across all maturities\")\n",
    "    print(\"and creates a yield curve visualization for market analysis.\")\n",
    "    print(\"\\nNote: You'll need to replace 'YOUR_FRED_API_KEY_HERE' with your actual FRED API key.\")\n",
    "    print(\"Get one free at: https://fred.stlouisfed.org/docs/api/api_key.html\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    analyze_treasury_yield_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de05167d",
   "metadata": {},
   "source": [
    "Asw ERP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f75b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import xlwings as xw\n",
    "import traceback\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_erp_data_to_excel():\n",
    "    \"\"\"\n",
    "    Scrapes Equity Risk Premium (ERP) data from Aswath Damodaran's NYU homepage.\n",
    "    Extracts both current and previous month implied ERP values, which are important\n",
    "    indicators for equity market valuation and investment decision-making.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Excel setup\n",
    "    book = xw.Book('Richspread.xlsx')\n",
    "    sheet = book.sheets['MarketSnapshot']\n",
    "    start_cell = \"B195\"\n",
    "    \n",
    "    # Clear the target area before writing new data\n",
    "    sheet.range(\"B194:B210\").clear()\n",
    "    \n",
    "    # URL for Professor Damodaran's finance data\n",
    "    url = \"https://pages.stern.nyu.edu/~adamodar/New_Home_Page/home.htm\"\n",
    "    erp_tables = {}\n",
    "    \n",
    "    try:\n",
    "        # Set up headers to mimic a real browser request\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        print(f\"Fetching ERP data from: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        print(\"Page content retrieved successfully.\")\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract current month ERP data\n",
    "        current_text = extract_current_erp(soup)\n",
    "        \n",
    "        # Extract previous month ERP data\n",
    "        prev_text = extract_previous_erp(soup)\n",
    "\n",
    "        # Store the extracted data\n",
    "        if current_text:\n",
    "            df = pd.DataFrame([[current_text, \"\"]], columns=[\"Raw Text\", \"\"])\n",
    "            erp_tables[\"Implied ERP (Current)\"] = df\n",
    "            print(\"Successfully stored current ERP data.\")\n",
    "        else:\n",
    "            print(\"Warning: Could not find current ERP section on the page.\")\n",
    "\n",
    "        if prev_text:\n",
    "            df = pd.DataFrame([[prev_text, \"\"]], columns=[\"Raw Text\", \"\"])\n",
    "            erp_tables[\"Implied ERP (Previous Month)\"] = df\n",
    "            print(\"Successfully stored previous month ERP data.\")\n",
    "        else:\n",
    "            print(\"Warning: Could not find previous month ERP section on the page.\")\n",
    "\n",
    "        # Export to Excel if we found any data\n",
    "        if erp_tables:\n",
    "            write_erp_tables_to_excel(erp_tables, sheet, start_cell)\n",
    "        else:\n",
    "            print(\"\\nNo ERP data was extracted. Nothing to write to Excel.\")\n",
    "        \n",
    "        return erp_tables\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error occurred during web request: {e}\")\n",
    "        traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def extract_current_erp(soup):\n",
    "    \"\"\"\n",
    "    Extracts the current month implied ERP text from the webpage.\n",
    "    Looks for text blocks starting with \"Implied ERP on...\"\n",
    "    \"\"\"\n",
    "    current_text = None\n",
    "    \n",
    "    # Search through all strong/bold tags for the current ERP section\n",
    "    strong_tags = soup.find_all('strong')\n",
    "    for tag in strong_tags:\n",
    "        tag_text = tag.get_text(strip=True)\n",
    "        if tag_text.startswith('Implied ERP on'):\n",
    "            # Get the full text from the parent element\n",
    "            current_text = tag.parent.get_text(separator=\" \", strip=True)\n",
    "            print(\"Found current ERP text block.\")\n",
    "            break\n",
    "    \n",
    "    return current_text\n",
    "\n",
    "def extract_previous_erp(soup):\n",
    "    \"\"\"\n",
    "    Extracts the previous month implied ERP text from the webpage.\n",
    "    Looks for text blocks starting with \"Implied ERP in previous month...\"\n",
    "    \"\"\"\n",
    "    prev_text = None\n",
    "    \n",
    "    # Search through all paragraph tags for the previous month ERP section\n",
    "    p_tags = soup.find_all('p')\n",
    "    for p in p_tags:\n",
    "        p_text = p.get_text(separator=\" \", strip=True)\n",
    "        if p_text.startswith('Implied ERP in previous month'):\n",
    "            prev_text = p_text\n",
    "            print(\"Found previous month ERP text block.\")\n",
    "            break\n",
    "    \n",
    "    return prev_text\n",
    "\n",
    "def cell_to_rowcol(cell_ref):\n",
    "    \"\"\"\n",
    "    Converts Excel cell reference (e.g., 'B195') to (row, col) tuple.\n",
    "    This helps with programmatic cell manipulation in Excel.\n",
    "    \"\"\"\n",
    "    match = re.match(r\"([A-Za-z]+)([0-9]+)\", cell_ref)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid cell reference: {cell_ref}\")\n",
    "    \n",
    "    col_letters, row = match.groups()\n",
    "    col = 0\n",
    "    \n",
    "    # Convert column letters to numeric value (A=1, B=2, etc.)\n",
    "    for char in col_letters.upper():\n",
    "        col = col * 26 + (ord(char) - ord('A') + 1)\n",
    "    \n",
    "    return int(row), col\n",
    "\n",
    "def get_next_empty_row(sheet, start_cell):\n",
    "    \"\"\"\n",
    "    Finds the next empty row in the Excel sheet starting from the given cell.\n",
    "    This prevents overwriting existing data.\n",
    "    \"\"\"\n",
    "    start_row, start_col = cell_to_rowcol(start_cell)\n",
    "    current_row = start_row\n",
    "    \n",
    "    # Keep checking rows until we find an empty one\n",
    "    while True:\n",
    "        if not sheet.cells(current_row, start_col).value:\n",
    "            return current_row\n",
    "        current_row += 1\n",
    "\n",
    "def write_erp_tables_to_excel(erp_tables, sheet, start_cell):\n",
    "    \"\"\"\n",
    "    Writes the extracted ERP data to Excel with proper formatting.\n",
    "    Creates organized sections for current and previous month data.\n",
    "    \"\"\"\n",
    "    print(\"\\nWriting ERP data to Excel...\")\n",
    "    \n",
    "    # Find the next empty row to avoid overwriting data\n",
    "    row = get_next_empty_row(sheet, start_cell)\n",
    "    _, col = cell_to_rowcol(start_cell)\n",
    "    \n",
    "    # Write each ERP table section\n",
    "    for key, df in erp_tables.items():\n",
    "        # Write section header\n",
    "        sheet.cells(row, col).value = key\n",
    "        sheet.cells(row, col).font.bold = True  # Make headers bold\n",
    "        row += 1\n",
    "        \n",
    "        # Write the data rows\n",
    "        for i in range(df.shape[0]):\n",
    "            for j in range(df.shape[1]):\n",
    "                cell_value = df.iloc[i, j]\n",
    "                sheet.cells(row, col + j).value = cell_value\n",
    "                # Enable text wrapping for long content\n",
    "                sheet.cells(row, col + j).api.WrapText = True\n",
    "            row += 1\n",
    "        \n",
    "        # Add spacing between sections\n",
    "        row += 1\n",
    "    \n",
    "    print(\"ERP data successfully written to Excel.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Equity Risk Premium (ERP) Data Scraper\")\n",
    "    print(\"=\" * 45)\n",
    "    print(\"This script extracts current equity risk premium data\")\n",
    "    print(\"from Professor Aswath Damodaran's NYU homepage.\")\n",
    "    print(\"ERP is a key metric for equity valuation and market analysis.\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    extract_erp_data_to_excel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8c7de",
   "metadata": {},
   "source": [
    "Fed Rate Probability Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef030035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page: https://www.investing.com/central-banks/fed-rate-monitor\n",
      "Waiting for page to load...\n",
      "Clicking cookie consent button...\n",
      "\n",
      "Processing meeting: Jul 30, 2025\n",
      "First meeting is already expanded, skipping click\n",
      "Found table with 3 rows\n",
      "\n",
      "Fed Rate Probability Table for Jul 30, 2025:\n",
      "   Target Rate Current Probability% Previous Day Probability%  \\\n",
      "0  4.00 - 4.25                 4.3%                      6.4%   \n",
      "1  4.25 - 4.50                95.7%                     93.6%   \n",
      "\n",
      "  Previous Week Probability%  \n",
      "0                       6.4%  \n",
      "1                      93.6%  \n",
      "\n",
      "Processing meeting: Sep 17, 2025\n",
      "Clicking to expand: Sep 17, 2025\n",
      "Found table with 4 rows\n",
      "\n",
      "Fed Rate Probability Table for Sep 17, 2025:\n",
      "   Target Rate Current Probability% Previous Day Probability%  \\\n",
      "0  3.75 - 4.00                 2.6%                      3.5%   \n",
      "1  4.00 - 4.25                59.9%                     54.2%   \n",
      "2  4.25 - 4.50                37.5%                     42.2%   \n",
      "\n",
      "  Previous Week Probability%  \n",
      "0                       3.6%  \n",
      "1                      55.4%  \n",
      "2                      41.0%  \n",
      "\n",
      "Processing meeting: Oct 29, 2025\n",
      "Clicking to expand: Oct 29, 2025\n",
      "Found table with 5 rows\n",
      "\n",
      "Fed Rate Probability Table for Oct 29, 2025:\n",
      "   Target Rate Current Probability% Previous Day Probability%  \\\n",
      "0  3.50 - 3.75                 1.4%                      1.9%   \n",
      "1  3.75 - 4.00                33.1%                     30.5%   \n",
      "2  4.00 - 4.25                48.0%                     47.9%   \n",
      "3  4.25 - 4.50                17.6%                     19.8%   \n",
      "\n",
      "  Previous Week Probability%  \n",
      "0                       2.2%  \n",
      "1                      34.6%  \n",
      "2                      46.8%  \n",
      "3                      16.5%  \n",
      "\n",
      "Processing meeting: Dec 10, 2025\n",
      "Clicking to expand: Dec 10, 2025\n",
      "Found table with 6 rows\n",
      "\n",
      "Fed Rate Probability Table for Dec 10, 2025:\n",
      "   Target Rate Current Probability% Previous Day Probability%  \\\n",
      "0  3.25 - 3.50                 0.9%                      1.2%   \n",
      "1  3.50 - 3.75                21.8%                     19.9%   \n",
      "2  3.75 - 4.00                42.7%                     41.4%   \n",
      "3  4.00 - 4.25                28.4%                     30.2%   \n",
      "4  4.25 - 4.50                 6.3%                      7.3%   \n",
      "\n",
      "  Previous Week Probability%  \n",
      "0                       1.5%  \n",
      "1                      24.5%  \n",
      "2                      43.0%  \n",
      "3                      25.9%  \n",
      "4                       5.1%  \n",
      "\n",
      "Writing data to Excel...\n",
      "Successfully wrote Fed Rate data to Excel starting at cell M70\n",
      "Browser closed\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import xlwings as xw\n",
    "\n",
    "def extract_fed_rate_tables_to_excel():\n",
    "    \"\"\"Extract Fed Rate Monitor probability tables and send to Excel in separate tables\"\"\"\n",
    "    url = \"https://www.investing.com/central-banks/fed-rate-monitor\"\n",
    "    \n",
    "    # Setup Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-logging\")\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "    chrome_options.add_argument(\"--silent\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\", \"enable-logging\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading page: {url}\")\n",
    "        driver.get(url)\n",
    "        print(\"Waiting for page to load...\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Handle cookie consent popup if it appears\n",
    "        try:\n",
    "            cookie_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'Accept') or contains(text(), 'accept') or contains(@id, 'cookie')]\")\n",
    "            print(\"Clicking cookie consent button...\")\n",
    "            cookie_button.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            print(\"No cookie consent popup found or it couldn't be clicked\")\n",
    "        \n",
    "        # Use the exact XPaths provided by the user\n",
    "        xpath_templates = [\n",
    "            \"/html/body/div[7]/section/div[4]/div[2]/label/div\",  # First meeting\n",
    "            \"/html/body/div[7]/section/div[4]/div[3]/label/div\",  # Second meeting\n",
    "            \"/html/body/div[7]/section/div[4]/div[4]/label/div\",  # Third meeting\n",
    "            \"/html/body/div[7]/section/div[4]/div[5]/label/div\"   # Fourth meeting \n",
    "        ]\n",
    "        \n",
    "        # Dictionary to store results for each meeting date\n",
    "        meeting_data = {}\n",
    "        \n",
    "        # Process each XPath\n",
    "        for i, xpath in enumerate(xpath_templates):\n",
    "            try:\n",
    "                # Try to find the element\n",
    "                try:\n",
    "                    label_div = driver.find_element(By.XPATH, xpath)\n",
    "                    \n",
    "                    # Get the meeting date from the div\n",
    "                    meeting_date = label_div.text.strip()\n",
    "                    if not meeting_date:\n",
    "                        meeting_date = f\"Meeting {i+1}\"\n",
    "                    \n",
    "                    print(f\"\\nProcessing meeting: {meeting_date}\")\n",
    "                    \n",
    "                    # Get the parent label element (for clicking)\n",
    "                    parent_xpath = xpath.rsplit('/', 1)[0]  # Remove the '/div' part to get the label\n",
    "                    parent_label = driver.find_element(By.XPATH, parent_xpath)\n",
    "                    \n",
    "                    # Scroll to the element\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", parent_label)\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    # Click to expand if this is not the first label (which is already expanded)\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            print(f\"Clicking to expand: {meeting_date}\")\n",
    "                            driver.execute_script(\"arguments[0].click();\", parent_label)\n",
    "                            time.sleep(2)  # Wait for expansion animation\n",
    "                        except Exception as e:\n",
    "                            print(f\"Could not click label: {e}\")\n",
    "                    else:\n",
    "                        print(f\"First meeting is already expanded, skipping click\")\n",
    "                    \n",
    "                    # Now find the table - it should be in the next sibling of the label\n",
    "                    try:\n",
    "                        # Use JavaScript to find the table\n",
    "                        table = driver.execute_script(\"\"\"\n",
    "                            var label = arguments[0];\n",
    "                            var container = label.nextElementSibling;\n",
    "                            if (container) {\n",
    "                                var tables = container.querySelectorAll('table');\n",
    "                                if (tables.length > 0) {\n",
    "                                    return tables[tables.length - 1]; // Get the last table (probability table)\n",
    "                                }\n",
    "                            }\n",
    "                            return null;\n",
    "                        \"\"\", parent_label)\n",
    "                        \n",
    "                        if not table:\n",
    "                            print(f\"No table found for {meeting_date} using JavaScript approach\")\n",
    "                            \n",
    "                            # Try to find it using XPath - look for the Target Rate table\n",
    "                            probability_table_xpath = f\"{parent_xpath}/following-sibling::div[1]//table[.//th[contains(text(), 'Target Rate')]]\"\n",
    "                            table = driver.find_element(By.XPATH, probability_table_xpath)\n",
    "                        \n",
    "                        if table:\n",
    "                            # Extract table data\n",
    "                            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                            print(f\"Found table with {len(rows)} rows\")\n",
    "                            \n",
    "                            # Extract headers and data\n",
    "                            headers = []\n",
    "                            data = []\n",
    "                            \n",
    "                            for j, row in enumerate(rows):\n",
    "                                cells = row.find_elements(By.TAG_NAME, \"th\") + row.find_elements(By.TAG_NAME, \"td\")\n",
    "                                row_data = [cell.text.strip() for cell in cells]\n",
    "                                \n",
    "                                if j == 0:  # Header row\n",
    "                                    headers = row_data\n",
    "                                else:  # Data rows\n",
    "                                    data.append(row_data)\n",
    "                            \n",
    "                            # Create DataFrame for this meeting\n",
    "                            meeting_df = pd.DataFrame(data, columns=headers)\n",
    "                            \n",
    "                            # Store in our results\n",
    "                            meeting_data[meeting_date] = meeting_df\n",
    "                            \n",
    "                            # Display results in console (keeping the console output format)\n",
    "                            print(f\"\\nFed Rate Probability Table for {meeting_date}:\")\n",
    "                            print(meeting_df)\n",
    "                        else:\n",
    "                            print(f\"No table found for {meeting_date}\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting table for {meeting_date}: {e}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Could not find element with XPath {xpath}: {e}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing XPath {i+1}: {e}\")\n",
    "        \n",
    "        # Write data to Excel in the console-like format\n",
    "        write_to_excel_separate_tables(meeting_data)\n",
    "        \n",
    "        return meeting_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Browser closed\")\n",
    "        return None\n",
    "\n",
    "def write_to_excel_separate_tables(meeting_data):\n",
    "    \"\"\"Write the meeting data to Excel at cell M70, with each meeting as a separate table\"\"\"\n",
    "    if not meeting_data:\n",
    "        print(\"No data to write to Excel\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nWriting data to Excel...\")\n",
    "        \n",
    "        # Connect to Excel - use active workbook or create a new one\n",
    "        try:\n",
    "            wb = xw.books.active\n",
    "        except:\n",
    "            wb = xw.Book()\n",
    "        \n",
    "        # Try to select the MarketSnapshot sheet or use the active sheet\n",
    "        try:\n",
    "            sheet = wb.sheets['MarketSnapshot']\n",
    "        except:\n",
    "            sheet = wb.sheets.active\n",
    "            print(f\"Using active sheet: {sheet.name}\")\n",
    "        # Start at cell M70\n",
    "        start_cell = sheet.range('M70')\n",
    "        current_row = start_cell.row\n",
    "        current_col = start_cell.column\n",
    "        \n",
    "        # Write each meeting's data as a separate table, one after another\n",
    "        for meeting_date, df in meeting_data.items():\n",
    "            # Write the meeting date as a title\n",
    "            title_cell = sheet.cells(current_row, current_col)\n",
    "            title_cell.value = f\"Fed Rate Probability Table for {meeting_date}:\"\n",
    "            title_cell.font.bold = True\n",
    "            current_row += 1\n",
    "            \n",
    "            # Write the headers\n",
    "            for col_idx, header in enumerate(df.columns):\n",
    "                header_cell = sheet.cells(current_row, current_col + col_idx)\n",
    "                header_cell.value = header\n",
    "                header_cell.font.bold = True\n",
    "            current_row += 1\n",
    "            \n",
    "            # Write the data rows\n",
    "            for _, row in df.iterrows():\n",
    "                for col_idx, value in enumerate(row):\n",
    "                    data_cell = sheet.cells(current_row, current_col + col_idx)\n",
    "                    data_cell.value = value\n",
    "                current_row += 1\n",
    "            \n",
    "            # Add an empty row between tables\n",
    "            current_row += 2\n",
    "        \n",
    "        print(f\"Successfully wrote Fed Rate data to Excel starting at cell M70\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to Excel: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_fed_rate_tables_to_excel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
